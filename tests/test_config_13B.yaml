base_dir: "/opt/product/"
encoder:
  clip_encoder:
    type: clip
    model_name_or_path: ${base_dir}/LLaVA/checkpoints/clip-vit-large-patch14
    freeze: true
    select_feature_layer: -2

dataset:
  cc3m:
    type: "cc3m"
    data_path: ${base_dir}/LLaVA/dataset/LLaVA-CC3M-Pretrain-595K/chat.json
    image_folder: ${base_dir}/LLaVA/dataset/LLaVA-CC3M-Pretrain-595K/images
    sample_ratio: 115
    # 最大样本比例，用于小样本测试
    max_sample_size: 1000
    # 用于设置预处理操作
    multimodal: true

projector:
  image_proj_13B:
    type: "linear"
    in_features: 1024
    out_features: 5120
    freeze: true
    ckpt_path: ${base_dir}/LLaVA/tools/13B_mm_projector.pt
  mlp_out_13B:
    type: "mlp"
    in_features: 5120
    out_features: 256
    freeze: false
    ckpt_path: ${base_dir}/llrs/checkpoints/mlp_13b.pt

llm:
  llama2_13b:
    type: llama2
    model_name_or_path: ${base_dir}/llama/llama-2-13b-chat-hf
    lora_enable: false
    freeze: true
  llava_13b:
    type: llava
    model_name_or_path: "xinlai/LISA-13B-llama2-v1"
    lora_enable: false
    freeze: true
    precision: "fp16"

decoder:
  sam_decoder_h:
    type: sam
    model_type: "sam_h"
    model_name_or_path: ${base_dir}/llrs/checkpoints/sam_13b.pt
    train_mask_decoder: true

tokenizer:
  llama2_13B:
    type: llama2
    model_name_or_path: "xinlai/LISA-13B-llama2-v1"
    # model_name_or_path: "xinlai/LISA-13B-llama2-v1-explanatory"

one_model:
  # 类型
  type: "one_model"
  dataset: ["cc3m"]
  encoder: clip_encoder
  in_projector: image_proj_13B
  tokenizer: llama2_13B
  llm: llava_13b
  out_projector: mlp_out_13B
  decoder: sam_decoder_h
  # 对话模版，取决于llm的类型
  conv_type: "plain"
  # 训练模式，0: pretrain (train encoder and projector), 1: finetune (train decoder and projector), 2: full train (train encoder and decoder projector)
  train_mode: 1
