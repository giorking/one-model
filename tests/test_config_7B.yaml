base_dir: "/opt/product/"
encoder:
  clip_encoder:
    type: clip
    model_name_or_path: "/opt/product/LLaVA/checkpoints/clip-vit-large-patch14"
    freeze: true
    select_feature_layer: -2

dataset:
  cc3m:
    type: "cc3m"
    dataset_dir: "/opt/product/LLaVA/dataset/LLaVA-CC3M-Pretrain-595K"
    # 最大样本比例，用于小样本测试
    max_sample_size: 1000

  hybrid_ds:
    type: "hybrid"
    dataset_dir: ${base_dir}/dataset
    vision_tower: ${base_dir}/LLaVA/checkpoints/clip-vit-large-patch14
    samples_per_epoch: 200
    num_classes_per_sample: 3
    exclude_val: false
    dataset_names: "sem_seg||refer_seg||reason_seg"
    sample_rates: "9,3,1"
    sem_seg_data: "ade20k"
    refer_seg_data: "refcoco+||refcocog"
    vqa_data: "llava_instruct_150k"
    reason_seg_data: "ReasonSeg|train"
    explanatory: 0.1

  hybrid_val_ds:
    type: "hybrid_val"
    dataset_dir: "${base_dir}/dataset"
    vision_tower: ${base_dir}/LLaVA/checkpoints/clip-vit-large-patch14
    dataset_names: "ReasonSeg|val"

projector:
  image_proj_7B:
    type: "linear"
    in_features: 1024
    out_features: 4096
    freeze: true
    ckpt_path: "/opt/product/LLaVA/tools/7B_mm_projector.pt"
  image_proj_13B:
    type: "linear"
    in_features: 1024
    out_features: 5120
    freeze: true
    ckpt_path: "/opt/product/LLaVA/tools/13B_mm_projector.pt"
  llm_out_proj_7B:
    type: "linear"
    in_features: 1024
    out_features: 4096
    freeze: false
    ckpt_path: "/opt/product/LLaVA/tools/7B_mm_projector.pt"
  mlp_out_7B:
    type: "mlp"
    in_features: 4096
    out_features: 256
    freeze: false
    ckpt_path: "/opt/product/llrs/checkpoints/mlp_7b.pt"
  mlp_out_13B:
    type: "mlp"
    in_features: 5120
    out_features: 256
    freeze: false
    ckpt_path: "/opt/product/llrs/checkpoints/mlp_13b.pt"

  dummy:
    type: "dummy"
    freeze: true

llm:
  llama2_7b:
    type: llama2
    model_name_or_path: "/opt/product/llama/llama-2-7b-chat-hf"
    lora_enable: false
    freeze: true

  llama2_13b:
    type: llama2
    model_name_or_path: "/opt/product/llama/llama-2-13b-chat-hf"
    lora_enable: false
    freeze: true

  llava_7b:
    type: llava
    model_name_or_path: "/opt/product/LLaVA/checkpoints/llava-7b-llama-2-7b-chat"
    lora_enable: false
    freeze: true

decoder:
  sam_decoder_l:
    type: sam
    model_type: "sam_l"
    model_name_or_path: "/root/.cache/ckpts/sam_model_vit_l.pth"
    train_mask_decoder: true
  sam_decoder_h:
    type: sam
    model_type: "sam_h"
    model_name_or_path: "/root/.cache/ckpts/sam_vit_h_4b8939.pth"
    train_mask_decoder: true

  openseed_decoder_s:
    type: openseed
    # config file for openseed_model build
    config_path: "/opt/product/newly/OpenSeeD/configs/openseed/openseed_swint_lang.yaml"
    # model state dict for openseed_model build
    model_name_or_path: "/opt/product/newly/OpenSeeD/ckpts/model_state_dict_swint_51.2ap.pt"

  stable_diffusion_decoder:
    type: sd
    # model state dict for openseed_model build
    # model_name_or_path: "runwayml/stable-diffusion-v1-5"
    model_name_or_path: "/root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9"

tokenizer:
  llama2:
    type: llama2
    model_name_or_path: "/opt/product/LLaVA/checkpoints/llava-7b-llama-2-7b-chat"
  llama2_13B:
    type: llama2
    model_name_or_path: "/opt/product/LLaVA/checkpoints/llava-7b-llama-2-7b-chat"

one_model:
  # 类型
  type: "one_model"
  model_name_or_path: "/opt/product/llama/llama-2-7b-chat-hf"
  dataset: ["cc3m"]
  encoder: clip_encoder
  in_projector: image_proj_7B
  tokenizer: llama2
  llm: llava_7b
  out_projector: llm_out_proj_7B
  decoder: sam_decoder_l
  # 对话模版，取决于llm的类型
  conv_type: "plain"
  # 训练模式，0: pretrain (train encoder and projector), 1: finetune (train decoder and projector), 2: full train (train encoder and decoder projector)
  train_mode: 1

trainer:
  task: image_text_pretrain
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-4
  min_lr: 8e-5
  warmup_lr: 1e-6

  weight_decay: 0.05
  max_epoch: 4
  batch_size_train: 64
  batch_size_eval: 64
  num_workers: 4
  warmup_steps: 5000
  iters_per_epoch: 5000
  output_dir: "output/minigpt4_stage1_pretrain"
  amp: True
  resume_ckpt_path: null
  evaluate: False
  train_splits: ["train"]
  bits: 8
