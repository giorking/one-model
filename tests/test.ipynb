{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/lisa/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-28 17:01:25,995] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3064216/2809877646.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "from loguru import logger\n",
    "from one_model.common.registries import (\n",
    "    LLM_MODEL_REGISTRY,\n",
    "    TOKENIZER_REGISTRY,\n",
    "    ENCODER_REGISTRY,\n",
    "    PROJECTOR_REGISTRY,\n",
    "    DECODER_REGISTRY,\n",
    ")\n",
    "from one_model.dataset import *\n",
    "import transformers\n",
    "from one_model.common.mm_utils import (\n",
    "    get_model_name_from_path,\n",
    "    load_image,\n",
    "    tokenizer_image_token,\n",
    ")\n",
    "\n",
    "from addict import Dict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from one_model.dataset import *\n",
    "from one_model.model.encoder import *\n",
    "from one_model.model.decoder import *\n",
    "from one_model.model.projector import *\n",
    "from one_model.model.tokenizer import *\n",
    "from addict import Dict\n",
    "from one_model.common.config import Config\n",
    "from one_model.model.llm import *\n",
    "import os\n",
    "from one_model.common.conversation import conv_templates, SeparatorStyle\n",
    "from one_model.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch' from '/root/anaconda3/envs/lisa/lib/python3.10/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload module\n",
    "# imp.reload(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/opt/product/one_model/tests/test_config_13B.yaml'\n",
    "cur_dir = '/opt/product/one_model/tests'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:02:00.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mfrom_config\u001b[0m:\u001b[36m545\u001b[0m - \u001b[1mllava model config LlavaConfig {\n",
      "  \"_name_or_path\": \"xinlai/LISA-13B-llama2-v1\",\n",
      "  \"architectures\": [\n",
      "    \"LISAForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"freeze_mm_mlp_adapter\": true,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"image_aspect_ratio\": \"square\",\n",
      "  \"image_grid_pinpoints\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13824,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mm_hidden_size\": 1024,\n",
      "  \"mm_resampler_type\": null,\n",
      "  \"mm_use_im_patch_token\": false,\n",
      "  \"mm_use_im_start_end\": true,\n",
      "  \"mm_vision_select_feature\": \"patch\",\n",
      "  \"mm_vision_select_layer\": -2,\n",
      "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\n",
      "  \"model_type\": \"llava\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 40,\n",
      "  \"out_dim\": 256,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretrain_mm_mlp_adapter\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"train_mask_decoder\": true,\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"tune_mm_mlp_adapter\": false,\n",
      "  \"tune_mm_vision_resampler\": false,\n",
      "  \"use_cache\": false,\n",
      "  \"use_mm_proj\": true,\n",
      "  \"vision_tower\": \"openai/clip-vit-large-patch14\",\n",
      "  \"vocab_size\": 32003\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[32m2023-09-28 17:02:21.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mseg token idx 32000\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:34<00:00, 11.36s/it]\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32003. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "\u001b[32m2023-09-28 17:03:09.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mmodel LlavaLlamaForCausalLM(\n",
      "  (model): LlavaLlamaModel(\n",
      "    (embed_tokens): Embedding(32003, 5120, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (1): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (2): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (3): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (4): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (6): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (7): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (8): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (9): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (12): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (13): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (14): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (15): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (16): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (17): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (18): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (19): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (21): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (22): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (23): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (24): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (25): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (26): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (27): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (28): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (29): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (30): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (32): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (33): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (34): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (35): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (36): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (37): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (38): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (39): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear8bitLt(in_features=5120, out_features=32003, bias=False)\n",
      ")\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:09.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.encoder.clip_encoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mselect_layer -2, select_feature patch\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:13.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mimage_proj_13B_config {'type': 'linear', 'in_features': 1024, 'out_features': 5120, 'freeze': True, 'ckpt_path': '/opt/product/LLaVA/tools/13B_mm_projector.pt'}\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:13.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mprojector LinerProjector(\n",
      "  (projector): Linear(in_features=1024, out_features=5120, bias=True)\n",
      ")\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:14.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mout_project_13B_config {'type': 'mlp', 'in_features': 5120, 'out_features': 256, 'freeze': False, 'ckpt_path': '/opt/product/llrs/checkpoints/mlp_13b.pt'}\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:14.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mout_projector MlpProjector(\n",
      "  (projector): Sequential(\n",
      "    (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=5120, out_features=256, bias=True)\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:14.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.decoder.sam_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1msam decoder init, model_name_or_path /root/.cache/ckpts/sam_vit_h_4b8939.pth, model_type sam_h\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:22.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mprompt [INST] <<SYS>>\n",
      "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\n",
      "<</SYS>>\n",
      "\n",
      "<im_start><image><im_end>\n",
      "describe the image [/INST]\u001b[0m\n",
      "/root/anaconda3/envs/lisa/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "\u001b[32m2023-09-28 17:03:22.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:23.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:25.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:25.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:25.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:25.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:25.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:25.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:26.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:26.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:26.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:26.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:27.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:27.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:27.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:27.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:28.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:28.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:28.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:28.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:29.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:29.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:29.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:29.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:30.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:30.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:30.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:30.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:31.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:31.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:31.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:31.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:32.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:32.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:32.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:32.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:33.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:33.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:33.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:33.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:33.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:33.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:34.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:34.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:34.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:34.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:35.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:35.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:35.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:35.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:36.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:36.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:36.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:36.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:37.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:37.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:37.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:37.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:38.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:38.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:38.604\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:38.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:39.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:39.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:39.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:39.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:40.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:40.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:40.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:40.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:41.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:41.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:41.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:41.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:41.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:41.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:42.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:42.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:42.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:42.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:43.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:43.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:43.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:43.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:44.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:44.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:44.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:44.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:45.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:45.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:45.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:45.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:46.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:46.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:46.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:46.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:47.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:47.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:47.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:47.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:48.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:48.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:48.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:48.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:49.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:49.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:49.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:49.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:50.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:50.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:50.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:50.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:51.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:51.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:51.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:51.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:52.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:52.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:52.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:52.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:53.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:53.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:53.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:53.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:54.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mhidden_states torch.Size([1, 381, 256])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:03:54.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mseg_token_mask shape torch.Size([1, 381])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "config: Config = Config(Dict(cfg_path=config_path))\n",
    "one_model_cfg = config.model_cfg\n",
    "tokenizer_cfg = config.tokenizer_cfg[one_model_cfg.tokenizer]\n",
    "tokenizer_cls = TOKENIZER_REGISTRY.get(tokenizer_cfg.type)\n",
    "tokenizer = tokenizer_cls.from_config(tokenizer_cfg).tokenizer\n",
    "\n",
    "llm_config = config.llm_cfg[one_model_cfg.llm]\n",
    "llm_cls = LLM_MODEL_REGISTRY.get(llm_config.type)\n",
    "model: LlavaLlamaForCausalLM = llm_cls.from_config(llm_config, tokenizer)\n",
    "assert model is not None\n",
    "logger.info(\"model {}\", model)\n",
    "\n",
    "conv_mode = \"llava_llama_2\"\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "\n",
    "clip_encoder_large_config = config.encoder_cfg[one_model_cfg.encoder]\n",
    "clip_encoder_cls = ENCODER_REGISTRY.get(clip_encoder_large_config.type)\n",
    "clip_encoder: CLIPEncoder = clip_encoder_cls.from_config(clip_encoder_large_config)\n",
    "clip_encoder = clip_encoder.cuda(0)\n",
    "\n",
    "image_proj_13B_config = config.projector_cfg[one_model_cfg.in_projector]\n",
    "projector_cls = PROJECTOR_REGISTRY.get(image_proj_13B_config.type)\n",
    "logger.info(\"image_proj_13B_config {}\", image_proj_13B_config)\n",
    "in_projector = projector_cls.from_config(image_proj_13B_config)\n",
    "logger.info(\"projector {}\", in_projector)\n",
    "in_projector = in_projector.cuda(0)\n",
    "\n",
    "out_project_13B_config = config.projector_cfg[one_model_cfg.out_projector]\n",
    "out_projector_cls = PROJECTOR_REGISTRY.get(out_project_13B_config.type)\n",
    "out_projector = out_projector_cls.from_config(out_project_13B_config)\n",
    "out_projector = out_projector.cuda(0)\n",
    "logger.info(\"out_project_13B_config {}\", out_project_13B_config)\n",
    "logger.info(\"out_projector {}\", out_projector)\n",
    "\n",
    "sam_decoder_config = config.decoder_cfg[one_model_cfg.decoder]\n",
    "sam_decoder_config.model_name_or_path = \"/opt/product/llrs/checkpoints/sam_13b.pt\"\n",
    "sam_decoder_cls = DECODER_REGISTRY.get(sam_decoder_config.type)\n",
    "sam_decoder: SamDecoder = sam_decoder_cls.from_config(sam_decoder_config)\n",
    "sam_decoder = sam_decoder.cuda(0)\n",
    "\n",
    "model.get_model().vision_tower = clip_encoder\n",
    "model.get_model().mm_projector = in_projector\n",
    "\n",
    "image_file = f\"{cur_dir}/view.jpg\"\n",
    "image_processor = clip_encoder.image_processor\n",
    "\n",
    "# image processor\n",
    "image = load_image(image_file)\n",
    "image_tensor = (\n",
    "    image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    .half()\n",
    "    .cuda()\n",
    ")\n",
    "\n",
    "inp = \"segment the lake\"\n",
    "if image is not None:\n",
    "    # first message\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        inp = (\n",
    "            DEFAULT_IM_START_TOKEN\n",
    "            + DEFAULT_IMAGE_TOKEN\n",
    "            + DEFAULT_IM_END_TOKEN\n",
    "            + \"\\n\"\n",
    "            + inp\n",
    "        )\n",
    "    else:\n",
    "        inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + inp\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    image = None\n",
    "else:\n",
    "    # later messages\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "logger.info(\"prompt {}\", prompt)\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")\n",
    "prompt_len = len(prompt)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        images=image_tensor,\n",
    "        max_new_tokens=512,\n",
    "        num_beams=1,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "output_hidden_states = outputs.hidden_states[-1]\n",
    "output_ids = outputs.sequences\n",
    "\n",
    "text_output = tokenizer.decode(output_ids[0, input_ids.shape[1] :]).strip()\n",
    "seg_token_mask = output_ids[:, 1:] == model.seg_token_idx\n",
    "seg_token_mask = seg_token_mask.to(device)\n",
    "# hack for IMAGE_TOKEN_INDEX (we suppose that there is only one image, and it is in the front)\n",
    "seg_token_mask = torch.cat(\n",
    "    [\n",
    "        torch.zeros((seg_token_mask.shape[0], 255))\n",
    "        .bool()\n",
    "        .to(seg_token_mask.device),\n",
    "        seg_token_mask,\n",
    "    ],\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "output_ids = output_ids[0][output_ids[0] != IMAGE_TOKEN_INDEX]\n",
    "# text_output = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "# logger.info(\"text_output {}\", text_output)\n",
    "# if len(text_output) > prompt_len:\n",
    "#     text_output = text_output[prompt_len - 1 :]\n",
    "# logger.info(\"text_output {}\", text_output)\n",
    "text_output = text_output.replace(\"\\n\", \"\").replace(\"</s>\", \"\").replace(\"  \", \" \")\n",
    "text_output = text_output.split(\"ASSISTANT: \")[-1]\n",
    "\n",
    "hidden_states = []\n",
    "\n",
    "hidden_states.append(out_projector(output_hidden_states))\n",
    "logger.info(\"hidden_states {}\", hidden_states[0].shape)\n",
    "logger.info(\"seg_token_mask shape {}\", seg_token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:04:07.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mhidden_states torch.Size([1, 381, 256])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:04:07.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mseg_token_mask shape torch.Size([1, 381])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"hidden_states {}\", hidden_states[0].shape)\n",
    "logger.info(\"seg_token_mask shape {}\", seg_token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:13:44.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.decoder.sam_decoder\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1msam decoder init, model_name_or_path /opt/product/llrs/checkpoints/sam_13b.pt, model_type sam_h\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# change sam \n",
    "sam_decoder_config = config.decoder_cfg[one_model_cfg.decoder]\n",
    "sam_decoder_config.model_name_or_path = \"/opt/product/llrs/checkpoints/sam_13b.pt\"\n",
    "sam_decoder_cls = DECODER_REGISTRY.get(sam_decoder_config.type)\n",
    "sam_decoder: SamDecoder = sam_decoder_cls.from_config(sam_decoder_config)\n",
    "sam_decoder = sam_decoder.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:09:54.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mprompt [INST] <<SYS>>\n",
      "You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\n",
      "<</SYS>>\n",
      "\n",
      "<im_start><image><im_end>\n",
      "segment the lake [/INST]\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:54.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:54.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:54.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:54.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:55.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:55.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:55.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:55.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:56.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mimages shape torch.Size([1, 3, 224, 224])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:56.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.llm.llava\u001b[0m:\u001b[36mencode_images\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mimage_features shape torch.Size([1, 256, 5120])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:56.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mhidden_states torch.Size([1, 323, 256])\u001b[0m\n",
      "\u001b[32m2023-09-28 17:09:56.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mseg_token_mask shape torch.Size([1, 323])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conv_mode = \"llava_llama_2\"\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "\n",
    "image = load_image(image_file)\n",
    "image_tensor = (\n",
    "    image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "    .half()\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "inp = \"segment the lake\"\n",
    "if image is not None:\n",
    "    # first message\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        inp = (\n",
    "            DEFAULT_IM_START_TOKEN\n",
    "            + DEFAULT_IMAGE_TOKEN\n",
    "            + DEFAULT_IM_END_TOKEN\n",
    "            + \"\\n\"\n",
    "            + inp\n",
    "        )\n",
    "    else:\n",
    "        inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + inp\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    image = None\n",
    "else:\n",
    "    # later messages\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "logger.info(\"prompt {}\", prompt)\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "prompt_len = len(prompt)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        images=image_tensor,\n",
    "        max_new_tokens=512,\n",
    "        num_beams=1,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "output_hidden_states = outputs.hidden_states[-1]\n",
    "output_ids = outputs.sequences\n",
    "\n",
    "text_output = tokenizer.decode(output_ids[0, input_ids.shape[1] :]).strip()\n",
    "seg_token_mask = output_ids[:, 1:] == model.seg_token_idx\n",
    "seg_token_mask = seg_token_mask.to(device)\n",
    "# hack for IMAGE_TOKEN_INDEX (we suppose that there is only one image, and it is in the front)\n",
    "seg_token_mask = torch.cat(\n",
    "    [\n",
    "        torch.zeros((seg_token_mask.shape[0], 255))\n",
    "        .bool()\n",
    "        .to(seg_token_mask.device),\n",
    "        seg_token_mask,\n",
    "    ],\n",
    "    dim=1,\n",
    ")\n",
    "\n",
    "output_ids = output_ids[0][output_ids[0] != IMAGE_TOKEN_INDEX]\n",
    "# text_output = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "# logger.info(\"text_output {}\", text_output)\n",
    "# if len(text_output) > prompt_len:\n",
    "#     text_output = text_output[prompt_len - 1 :]\n",
    "# logger.info(\"text_output {}\", text_output)\n",
    "text_output = text_output.replace(\"\\n\", \"\").replace(\"</s>\", \"\").replace(\"  \", \" \")\n",
    "text_output = text_output.split(\"ASSISTANT: \")[-1]\n",
    "\n",
    "hidden_states = []\n",
    "\n",
    "hidden_states.append(out_projector(output_hidden_states))\n",
    "logger.info(\"hidden_states {}\", hidden_states[0].shape)\n",
    "logger.info(\"seg_token_mask shape {}\", seg_token_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "An object named 'sam' was already registered in 'Decoder' registry!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/product/one_model/demo.ipynb 单元格 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu_vp3/opt/product/one_model/demo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mone_model\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecoder\u001b[39;00m \u001b[39mimport\u001b[39;00m sam_decoder\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu_vp3/opt/product/one_model/demo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m imp\u001b[39m.\u001b[39;49mreload(sam_decoder)\n",
      "File \u001b[0;32m~/anaconda3/envs/lisa/lib/python3.10/imp.py:315\u001b[0m, in \u001b[0;36mreload\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreload\u001b[39m(module):\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"**DEPRECATED**\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[39m    Reload the module and return it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mreload(module)\n",
      "File \u001b[0;32m~/anaconda3/envs/lisa/lib/python3.10/importlib/__init__.py:169\u001b[0m, in \u001b[0;36mreload\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mspec not found for the module \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m--> 169\u001b[0m _bootstrap\u001b[39m.\u001b[39;49m_exec(spec, module)\n\u001b[1;32m    170\u001b[0m \u001b[39m# The module may have replaced itself in sys.modules!\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39mreturn\u001b[39;00m sys\u001b[39m.\u001b[39mmodules[name]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:619\u001b[0m, in \u001b[0;36m_exec\u001b[0;34m(spec, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/product/one_model/one_model/model/decoder/sam_decoder.py:14\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mone_model\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprocessor\u001b[39;00m \u001b[39mimport\u001b[39;00m SamPreProcessor\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mone_model\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloss\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m@DECODER_REGISTRY\u001b[39;49m\u001b[39m.\u001b[39;49mregister(alias\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msam\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m---> 14\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mSamDecoder\u001b[39;49;00m(nn\u001b[39m.\u001b[39;49mModule):\n\u001b[1;32m     15\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, model_name_or_path, model_type, train_mask_decoder) \u001b[39m-\u001b[39;49m\u001b[39m>\u001b[39;49m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m     16\u001b[0m         \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m()\n",
      "File \u001b[0;32m/opt/product/one_model/one_model/common/registry.py:60\u001b[0m, in \u001b[0;36mRegistry.register.<locals>.deco\u001b[0;34m(func_or_class)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     name \u001b[39m=\u001b[39m func_or_class\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_register(name, func_or_class)\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m func_or_class\n",
      "File \u001b[0;32m/opt/product/one_model/one_model/common/registry.py:42\u001b[0m, in \u001b[0;36mRegistry._do_register\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_register\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m---> 42\u001b[0m         name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj_map\n\u001b[1;32m     43\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mAn object named \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m was already registered in \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m registry!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     44\u001b[0m         name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     \u001b[39m# register ignore lower/upper case\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obj_map[name\u001b[39m.\u001b[39mlower()] \u001b[39m=\u001b[39m obj\n",
      "\u001b[0;31mAssertionError\u001b[0m: An object named 'sam' was already registered in 'Decoder' registry!"
     ]
    }
   ],
   "source": [
    "# from one_model.model.decoder import sam_decoder\n",
    "# imp.reload(sam_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:14:05.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mone_model.model.decoder.sam_decoder\u001b[0m:\u001b[36mget_visual_embs_img_paths\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mpixel_value.shape torch.Size([3, 1024, 1024])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test sam decoder\n",
    "decoder_result = sam_decoder.forward(\n",
    "        image_paths=[image_file],\n",
    "        hidden_states=hidden_states,\n",
    "        gt_masks=None,\n",
    "        inference=True,\n",
    "        seg_token_mask=seg_token_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_masks': [tensor([[[-1.1943e+01, -1.1939e+01, -1.1827e+01,  ..., -1.0311e+01,\n",
       "            -1.0178e+01, -1.0174e+01],\n",
       "           [-1.1939e+01, -1.1935e+01, -1.1823e+01,  ..., -1.0312e+01,\n",
       "            -1.0181e+01, -1.0176e+01],\n",
       "           [-1.1820e+01, -1.1817e+01, -1.1712e+01,  ..., -1.0350e+01,\n",
       "            -1.0256e+01, -1.0253e+01],\n",
       "           ...,\n",
       "           [ 5.3950e-01,  5.4195e-01,  6.1592e-01,  ...,  3.0288e-01,\n",
       "             1.3493e-01,  1.2936e-01],\n",
       "           [ 2.4756e-03,  4.7638e-03,  7.3663e-02,  ..., -2.0243e-01,\n",
       "            -3.4724e-01, -3.5204e-01],\n",
       "           [-8.8530e-01, -8.8333e-01, -8.2416e-01,  ..., -1.1265e+00,\n",
       "            -1.2560e+00, -1.2603e+00]]], device='cuda:0',\n",
       "         grad_fn=<SelectBackward0>)],\n",
       " 'gt_masks': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:14:12.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mpred_masks 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-28 17:14:12.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1msave segment to ./vis/view.jpg\u001b[0m\n",
      "\u001b[32m2023-09-28 17:14:12.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1minfer text out Sure,[SEG] .\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'prompt': '[INST] <<SYS>>\\nYou are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\\n<</SYS>>\\n\\n<im_start><image><im_end>\\nsegment the lake [/INST]', 'outputs': 'Sure,[SEG] .'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", {\"prompt\": prompt, \"outputs\": text_output}, \"\\n\")\n",
    "save_img = None\n",
    "pred_masks = decoder_result[\"pred_masks\"]\n",
    "image_np = cv2.imread(image_file)\n",
    "image_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "logger.info(\"pred_masks {}\", len(pred_masks))\n",
    "for i, pred_mask in enumerate(pred_masks):\n",
    "    if pred_mask.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    pred_mask = pred_mask.detach().cpu().numpy()[0]\n",
    "    pred_mask = pred_mask > 0\n",
    "\n",
    "    save_img = image_np.copy()\n",
    "    save_img[pred_mask] = (\n",
    "        image_np * 0.5\n",
    "        + pred_mask[:, :, None].astype(np.uint8) * np.array([255, 0, 0]) * 0.5\n",
    "    )[pred_mask]\n",
    "image_name = Path(image_file).name\n",
    "vis_save_path = \"./vis\"\n",
    "if save_img is not None:\n",
    "    save_path = f\"{vis_save_path}/{image_name}\"\n",
    "    logger.info(\"save segment to {}\", save_path)\n",
    "    cv2.imwrite(save_path, save_img[:, :, ::-1])\n",
    "logger.info(\"infer text out {}\", text_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
