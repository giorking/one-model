2023-10-30 13:27:39.470 | INFO     | one_model.model.one_model:__init__:41 - current select device 0
'(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /xinlai/LISA-13B-llama2-v1/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f73dea9f2b0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: fd7f9c36-8b72-4cce-8cbd-34f4b77bb53a)')' thrown while requesting HEAD https://huggingface.co/xinlai/LISA-13B-llama2-v1/resolve/main/tokenizer_config.json
'(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /xinlai/LISA-13B-llama2-v1/resolve/main/tokenizer.json (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f73dea540a0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 49e7141a-3a62-4cf4-9e92-d8cbd900a28a)')' thrown while requesting HEAD https://huggingface.co/xinlai/LISA-13B-llama2-v1/resolve/main/tokenizer.json
'(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /xinlai/LISA-13B-llama2-v1/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f73dea9fbb0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: c1aa3fbc-f85f-44fd-82b3-04e8232dde4b)')' thrown while requesting HEAD https://huggingface.co/xinlai/LISA-13B-llama2-v1/resolve/main/config.json
2023-10-30 13:28:09.980 | INFO     | one_model.model.llm.llava:from_config:520 - llava model config LlavaConfig {
  "_name_or_path": "xinlai/LISA-13B-llama2-v1",
  "architectures": [
    "LISAForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": true,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "image_aspect_ratio": "square",
  "image_grid_pinpoints": null,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": true,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14",
  "model_type": "llava",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "out_dim": 256,
  "pad_token_id": 0,
  "pretrain_mm_mlp_adapter": null,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "train_mask_decoder": true,
  "transformers_version": "4.33.2",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "use_cache": false,
  "use_mm_proj": true,
  "vision_tower": "openai/clip-vit-large-patch14",
  "vocab_size": 32003
}

2023-10-30 13:28:09.981 | INFO     | one_model.model.llm.llava:from_config:527 - load_in_8bit True device 0, torch_dtype torch.float16
2023-10-30 13:28:09.985 | INFO     | one_model.model.llm.llava:from_config:555 - seg_token_idx 32000
'(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /xinlai/LISA-13B-llama2-v1/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f73dd53e220>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: f9d69c71-0eee-431d-b247-561cbb17270d)')' thrown while requesting HEAD https://huggingface.co/xinlai/LISA-13B-llama2-v1/resolve/main/config.json
'(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /xinlai/LISA-13B-llama2-v1/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f73dd53e640>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: b22ea67d-ffdf-47bf-9b58-d46defdb8bc7)')' thrown while requesting HEAD https://huggingface.co/xinlai/LISA-13B-llama2-v1/resolve/main/config.json
2023-10-30 13:28:30.137 | INFO     | one_model.model.llm.llava:__init__:44 - seg token idx 32000
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:42<01:24, 42.22s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:20<00:40, 40.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:53<00:00, 36.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:53<00:00, 37.92s/it]
'(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /xinlai/LISA-13B-llama2-v1/resolve/main/generation_config.json (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x7f73de035fa0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 799323d7-159c-4cbc-b990-89015af5a1fa)')' thrown while requesting HEAD https://huggingface.co/xinlai/LISA-13B-llama2-v1/resolve/main/generation_config.json
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32003. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
2023-10-30 13:30:35.105 | INFO     | one_model.model.encoder.clip_encoder:__init__:20 - select_layer -2, select_feature patch
2023-10-30 13:30:55.779 | INFO     | one_model.model.one_model:init_model:71 - iin_projector_config {'type': 'linear', 'in_features': 1024, 'out_features': 5120, 'freeze': True, 'ckpt_path': '${base_dir}/LLaVA/tools/13B_mm_projector.pt'}
2023-10-30 13:30:55.901 | INFO     | one_model.model.one_model:init_model:73 - projector LinerProjector(
  (projector): Linear(in_features=1024, out_features=5120, bias=True)
)
2023-10-30 13:30:56.518 | INFO     | one_model.model.decoder.sam_decoder:__init__:20 - sam decoder init, model_name_or_path /home/luban/model_ckpts/llrs/checkpoints/sam_13b.pt, model_type sam_h
/home/luban/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/luban/model_ckpts/LLaVA/checkpoints/clip-vit-large-patch14/preprocessor_config.json
self.task_switch  {'coco': True, 'o365': False}
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loading pipeline components...:  29%|██▊       | 2/7 [00:02<00:05,  1.12s/it]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Loading pipeline components...:  43%|████▎     | 3/7 [00:06<00:09,  2.49s/it]Loading pipeline components...:  57%|█████▋    | 4/7 [00:06<00:04,  1.60s/it]Loading pipeline components...:  71%|███████▏  | 5/7 [00:08<00:03,  1.53s/it]Loading pipeline components...:  86%|████████▌ | 6/7 [00:19<00:04,  4.85s/it]Loading pipeline components...: 100%|██████████| 7/7 [00:19<00:00,  2.82s/it]
2023-10-30 13:32:25.060 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe visible details about these objects and describe the interrelationships among these objects. [/INST]
/home/luban/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )
  warnings.warn(
2023-10-30 13:33:34.753 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 256])
2023-10-30 13:33:34.754 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-10-30 13:33:35.179 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:33:35.654 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:33:35.720 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 5120])
2023-10-30 13:33:35.720 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-10-30 13:33:35.874 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:33:35.884 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:33:35.885 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 5120])
2023-10-30 13:33:35.885 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-10-30 13:33:36.168 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:33:36.176 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-10-30 13:33:36.176 | INFO     | one_model.inference.infer:predict:33 - prompt describe visible details about these objects and describe the interrelationships among these objects., text output In the image, there are several objects visible on the road, including a large truck, a car, a pile of tires, and a traffic light. The truck is the most prominent vehicle on the road, taking up a significant portion of the scene. The car is positioned behind the truck, and the traffic light is located near the left side of the image. The pile of tires is situated on the right side of the road, possibly indicating a tire repair or disposal area.The interrelationship among these objects suggests that the truck and car are both using the road for transportation, while the traffic light helps to regulate the flow of traffic. The presence of the pile of tires indicates that there might be a nearby tire repair or waste disposal facility, which could be related to the truck or other vehicles on the road. Overall, the scene depicts a typical day on a road with various vehicles and objects in motion.
[ WARN:0@361.489] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
[ WARN:0@532.771] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:36:27.655 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate a building [/INST]
2023-10-30 13:36:29.292 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 326, 256])
2023-10-30 13:36:29.292 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 326])
2023-10-30 13:36:30.374 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:36:30.381 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:36:30.381 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 326, 5120])
2023-10-30 13:36:30.382 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 326])
2023-10-30 13:36:30.490 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:36:30.491 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:36:30.491 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 326, 5120])
2023-10-30 13:36:30.492 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 326])
2023-10-30 13:36:30.675 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
/home/luban/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9/feature_extractor/preprocessor_config.json
Running on local URL:  http://0.0.0.0:8085

To create a public link, set `share=True` in `launch()`.
input_str:  describe visible details about these objects and describe the interrelationships among these objects. input_image:  /tmp/gradio/05e1b79505597aa67cf8316375fc11920797a924/image.png
image_id:  b32342a4-76e5-11ee-99f1-1070fd901efa
input_str:  generate a building input_image:  None
image_id:  45700ee4-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:09,  5.00it/s]  6%|▌         | 3/50 [00:00<00:04, 10.21it/s] 10%|█         | 5/50 [00:00<00:03, 12.82it/s] 14%|█▍        | 7/50 [00:00<00:03, 14.27it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.16it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.72it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.10it/s] 30%|███       | 15/50 [00:01<00:02, 16.34it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.50it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.61it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.55it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.57it/s] 50%|█████     | 25/50 [00:01<00:01, 16.62it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.71it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.71it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.73it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.72it/s] 70%|███████   | 35/50 [00:02<00:00, 16.76it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.79it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.82it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.63it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.70it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.60it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.68it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.62it/s]100%|██████████| 50/50 [00:03<00:00, 15.97it/s]
2023-10-30 13:36:33.956 | INFO     | one_model.inference.infer:predict:33 - prompt generate a building, text output Sure,[SEG] .
2023-10-30 13:36:33.957 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/45700ee4-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@584.457] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:37:19.332 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
commercial photography cat, background white [/INST]
2023-10-30 13:37:20.988 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-10-30 13:37:20.988 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-10-30 13:37:21.487 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
2023-10-30 13:37:21.488 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:37:21.489 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-10-30 13:37:21.489 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-10-30 13:37:21.682 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
2023-10-30 13:37:21.683 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:37:21.683 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-10-30 13:37:21.684 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-10-30 13:37:21.881 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
input_str:  commercial photography cat, background white input_image:  None
image_id:  643ec6d0-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  8.15it/s]  6%|▌         | 3/50 [00:00<00:03, 13.11it/s] 10%|█         | 5/50 [00:00<00:03, 14.77it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.57it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.02it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.30it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.43it/s] 30%|███       | 15/50 [00:00<00:02, 16.56it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.64it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.69it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.73it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.76it/s] 50%|█████     | 25/50 [00:01<00:01, 16.78it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.79it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.80it/s] 62%|██████▏   | 31/50 [00:01<00:01, 15.60it/s] 66%|██████▌   | 33/50 [00:02<00:01, 15.94it/s] 70%|███████   | 35/50 [00:02<00:00, 16.20it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.37it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.48it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.56it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.62it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.66it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.69it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.71it/s]100%|██████████| 50/50 [00:03<00:00, 16.27it/s]
2023-10-30 13:37:25.093 | INFO     | one_model.inference.infer:predict:33 - prompt commercial photography cat, background white, text output Sure,[SEG] .
2023-10-30 13:37:25.093 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/643ec6d0-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@611.405] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/routes.py", line 442, in run_predict
    output = await app.get_blocks().process_api(
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1395, in process_api
    data = self.postprocess_data(fn_index, result["prediction"], state)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1329, in postprocess_data
    prediction_value = block.postprocess(prediction_value)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/components/image.py", line 320, in postprocess
    raise ValueError("Cannot process this value as an Image")
ValueError: Cannot process this value as an Image
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/routes.py", line 442, in run_predict
    output = await app.get_blocks().process_api(
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1395, in process_api
    data = self.postprocess_data(fn_index, result["prediction"], state)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1329, in postprocess_data
    prediction_value = block.postprocess(prediction_value)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/components/image.py", line 320, in postprocess
    raise ValueError("Cannot process this value as an Image")
ValueError: Cannot process this value as an Image
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/routes.py", line 442, in run_predict
    output = await app.get_blocks().process_api(
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1395, in process_api
    data = self.postprocess_data(fn_index, result["prediction"], state)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1329, in postprocess_data
    prediction_value = block.postprocess(prediction_value)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/components/image.py", line 320, in postprocess
    raise ValueError("Cannot process this value as an Image")
ValueError: Cannot process this value as an Image
2023-10-30 13:37:46.278 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
four pictures of commercial photography cat, background white [/INST]
2023-10-30 13:37:47.922 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-10-30 13:37:47.922 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:37:48.274 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
2023-10-30 13:37:48.277 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:37:48.277 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-10-30 13:37:48.277 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:37:48.472 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
2023-10-30 13:37:48.474 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:37:48.475 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-10-30 13:37:48.475 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:37:48.669 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
input_str:  4 pictures of commercial photography cat, background white input_image:  None
input_str:  4 pictures of commercial photography cat, background white input_image:  None
input_str:  4 pictures of commercial photography cat, background white input_image:  None
input_str:  four pictures of commercial photography cat, background white input_image:  None
image_id:  744eb814-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.32it/s]  6%|▌         | 3/50 [00:00<00:03, 13.11it/s] 10%|█         | 5/50 [00:00<00:03, 14.75it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.44it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.95it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.25it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.46it/s] 30%|███       | 15/50 [00:00<00:02, 16.59it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.64it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.72it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.77it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.81it/s] 50%|█████     | 25/50 [00:01<00:01, 16.75it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.76it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.75it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.78it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.79it/s] 70%|███████   | 35/50 [00:02<00:00, 16.80it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.59it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.65it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.63it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.70it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.73it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.75it/s] 98%|█████████▊| 49/50 [00:02<00:00, 16.76it/s]100%|██████████| 50/50 [00:03<00:00, 16.41it/s]
2023-10-30 13:37:51.842 | INFO     | one_model.inference.infer:predict:33 - prompt four pictures of commercial photography cat, background white, text output Sure,[SEG] .
2023-10-30 13:37:51.842 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/744eb814-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@627.286] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:38:02.152 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
four pictures of commercial photography cat, background black [/INST]
2023-10-30 13:38:03.812 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-10-30 13:38:03.813 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:38:03.887 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:38:04.452 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:38:04.538 | INFO     | one_model.inference.infer:predict:33 - prompt four pictures of commercial photography cat, background black, text output Sure,[SEG] .
2023-10-30 13:38:04.539 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/7dc5e14c-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@637.901] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:38:12.770 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
four pictures of commercial photography cat, background yellow [/INST]
2023-10-30 13:38:14.430 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-10-30 13:38:14.430 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:38:14.482 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
2023-10-30 13:38:14.490 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:38:14.490 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-10-30 13:38:14.490 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:38:14.689 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
2023-10-30 13:38:14.772 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:38:14.772 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-10-30 13:38:14.772 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:38:14.973 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt create a picture
input_str:  four pictures of commercial photography cat, background black input_image:  None
image_id:  7dc5e14c-76e6-11ee-99f1-1070fd901efa
input_str:  four pictures of commercial photography cat, background yellow input_image:  None
image_id:  8419b28a-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:08,  5.80it/s]  6%|▌         | 3/50 [00:00<00:04, 11.25it/s] 10%|█         | 5/50 [00:00<00:03, 13.50it/s] 14%|█▍        | 7/50 [00:00<00:02, 14.69it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.39it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.82it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.12it/s] 30%|███       | 15/50 [00:01<00:02, 16.34it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.47it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.55it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.61it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.66it/s] 50%|█████     | 25/50 [00:01<00:01, 16.70it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.56it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.64it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.68it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.72it/s] 70%|███████   | 35/50 [00:02<00:00, 16.75it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.78it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.79it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.79it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.77it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.65it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.62it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.25it/s]100%|██████████| 50/50 [00:03<00:00, 16.05it/s]
2023-10-30 13:38:18.219 | INFO     | one_model.inference.infer:predict:33 - prompt four pictures of commercial photography cat, background yellow, text output Sure,[SEG] .
2023-10-30 13:38:18.219 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/8419b28a-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@660.530] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:38:35.401 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
commercial photography cat, background yellow [/INST]
2023-10-30 13:38:37.702 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-10-30 13:38:37.703 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-10-30 13:38:37.771 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the picture
2023-10-30 13:38:38.212 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:38:38.297 | INFO     | one_model.inference.infer:predict:33 - prompt commercial photography cat, background yellow, text output Sure, it is[SEG] .
2023-10-30 13:38:38.297 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/919692e8-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@671.766] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:38:46.634 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
commercial photography elephant [/INST]
2023-10-30 13:38:49.016 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-10-30 13:38:49.016 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:38:49.077 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:38:49.545 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:38:49.638 | INFO     | one_model.inference.infer:predict:33 - prompt commercial photography elephant, text output Sure, it is[SEG] .
2023-10-30 13:38:49.639 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/98490cf6-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@685.216] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:39:00.090 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate elephant [/INST]
2023-10-30 13:39:01.794 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 327, 256])
2023-10-30 13:39:01.794 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 327])
2023-10-30 13:39:01.969 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:01.971 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:39:01.972 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 327, 5120])
2023-10-30 13:39:01.972 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 327])
2023-10-30 13:39:02.080 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:02.082 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:39:02.083 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 327, 5120])
2023-10-30 13:39:02.083 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 327])
2023-10-30 13:39:02.268 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  commercial photography cat, background yellow input_image:  None
image_id:  919692e8-76e6-11ee-99f1-1070fd901efa
input_str:  commercial photography elephant input_image:  None
image_id:  98490cf6-76e6-11ee-99f1-1070fd901efa
input_str:  generate elephant input_image:  None
image_id:  a04d5fb0-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:08,  5.62it/s]  6%|▌         | 3/50 [00:00<00:04, 11.10it/s] 10%|█         | 5/50 [00:00<00:03, 13.43it/s] 14%|█▍        | 7/50 [00:00<00:02, 14.68it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.43it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.89it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.18it/s] 30%|███       | 15/50 [00:01<00:02, 16.39it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.44it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.56it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.65it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.33it/s] 50%|█████     | 25/50 [00:01<00:01, 16.41it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.53it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.63it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.68it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.40it/s] 70%|███████   | 35/50 [00:02<00:00, 16.43it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.40it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.29it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.44it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.54it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.57it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.61it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.65it/s]100%|██████████| 50/50 [00:03<00:00, 15.99it/s]
2023-10-30 13:39:05.522 | INFO     | one_model.inference.infer:predict:33 - prompt generate elephant, text output Sure,[SEG] .
2023-10-30 13:39:05.523 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/a04d5fb0-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@701.427] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:39:16.299 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography elephant [/INST]
2023-10-30 13:39:17.953 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-10-30 13:39:17.953 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:17.986 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:17.988 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:39:17.989 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 5120])
2023-10-30 13:39:17.989 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:18.174 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:18.175 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:39:18.176 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 5120])
2023-10-30 13:39:18.176 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:18.284 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography elephant input_image:  None
image_id:  a9f6ddb6-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.42it/s]  6%|▌         | 3/50 [00:00<00:03, 13.39it/s] 10%|█         | 5/50 [00:00<00:03, 14.33it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.30it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.81it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.08it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.31it/s] 30%|███       | 15/50 [00:00<00:02, 16.43it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.50it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.55it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.46it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.55it/s] 50%|█████     | 25/50 [00:01<00:01, 16.62it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.63it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.67it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.70it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.73it/s] 70%|███████   | 35/50 [00:02<00:00, 16.74it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.75it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.76it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.75it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.77it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.79it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.71it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.51it/s]100%|██████████| 50/50 [00:03<00:00, 16.31it/s]
2023-10-30 13:39:21.555 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography elephant, text output Sure,[SEG] .
2023-10-30 13:39:21.555 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/a9f6ddb6-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@712.135] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:39:27.000 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography elephant [/INST]
2023-10-30 13:39:28.627 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-10-30 13:39:28.627 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:28.670 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:28.671 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:39:28.671 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 5120])
2023-10-30 13:39:28.672 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:28.782 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:28.784 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:39:28.784 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 5120])
2023-10-30 13:39:28.784 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:28.969 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography elephant input_image:  None
image_id:  b058e410-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.31it/s]  6%|▌         | 3/50 [00:00<00:03, 13.31it/s] 10%|█         | 5/50 [00:00<00:03, 14.80it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.60it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.04it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.06it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.10it/s] 30%|███       | 15/50 [00:00<00:02, 16.16it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.25it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.35it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.43it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.36it/s] 50%|█████     | 25/50 [00:01<00:01, 15.99it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.23it/s] 58%|█████▊    | 29/50 [00:01<00:01, 15.94it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.05it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.10it/s] 70%|███████   | 35/50 [00:02<00:00, 16.31it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.45it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.43it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.18it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.38it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.36it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.49it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.57it/s]100%|██████████| 50/50 [00:03<00:00, 16.07it/s]
2023-10-30 13:39:32.212 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography elephant, text output Sure,[SEG] .
2023-10-30 13:39:32.212 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/b058e410-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@721.360] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:39:36.224 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography elephant [/INST]
2023-10-30 13:39:37.845 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-10-30 13:39:37.845 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:37.878 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:37.879 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:39:37.880 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 5120])
2023-10-30 13:39:37.880 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:37.990 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:37.991 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:39:37.991 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 5120])
2023-10-30 13:39:37.992 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-10-30 13:39:38.175 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography elephant input_image:  None
image_id:  b5d879b4-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.31it/s]  6%|▌         | 3/50 [00:00<00:03, 13.26it/s] 10%|█         | 5/50 [00:00<00:03, 14.85it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.60it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.99it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.21it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.38it/s] 30%|███       | 15/50 [00:00<00:02, 16.48it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.44it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.53it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.60it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.64it/s] 50%|█████     | 25/50 [00:01<00:01, 16.67it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.16it/s] 58%|█████▊    | 29/50 [00:01<00:01, 15.82it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.09it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.11it/s] 70%|███████   | 35/50 [00:02<00:00, 16.29it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.24it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.39it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.39it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.49it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.53it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.58it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.62it/s]100%|██████████| 50/50 [00:03<00:00, 16.18it/s]
2023-10-30 13:39:41.394 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography elephant, text output Sure,[SEG] .
2023-10-30 13:39:41.395 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/b5d879b4-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@733.631] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:39:48.495 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography banana [/INST]
2023-10-30 13:39:50.756 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 256])
2023-10-30 13:39:50.757 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-10-30 13:39:50.874 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:50.876 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:39:50.876 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 5120])
2023-10-30 13:39:50.876 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-10-30 13:39:51.075 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:39:51.077 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:39:51.077 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 5120])
2023-10-30 13:39:51.077 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-10-30 13:39:51.268 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography banana input_image:  None
image_id:  bd28e424-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.43it/s]  6%|▌         | 3/50 [00:00<00:03, 13.36it/s] 10%|█         | 5/50 [00:00<00:03, 14.93it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.66it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.07it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.33it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.49it/s] 30%|███       | 15/50 [00:00<00:02, 16.59it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.65it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.69it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.72it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.69it/s] 50%|█████     | 25/50 [00:01<00:01, 16.72it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.63it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.67it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.70it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.64it/s] 70%|███████   | 35/50 [00:02<00:00, 16.49it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.58it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.64it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.36it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.48it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.56it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.47it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.56it/s]100%|██████████| 50/50 [00:03<00:00, 16.34it/s]
2023-10-30 13:39:54.457 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography banana, text output Sure, it is[SEG] .
2023-10-30 13:39:54.457 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/bd28e424-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@751.448] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:40:06.317 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography panda eating banana [/INST]
2023-10-30 13:40:08.597 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 256])
2023-10-30 13:40:08.597 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:40:08.968 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:40:08.970 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:40:08.971 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-10-30 13:40:08.971 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:40:09.086 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:40:09.088 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:40:09.088 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-10-30 13:40:09.088 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:40:09.278 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography panda eating banana input_image:  None
image_id:  c7c78d2c-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.28it/s]  6%|▌         | 3/50 [00:00<00:03, 13.25it/s] 10%|█         | 5/50 [00:00<00:03, 14.80it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.46it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.91it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.19it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.38it/s] 30%|███       | 15/50 [00:00<00:02, 16.48it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.54it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.60it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.46it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.54it/s] 50%|█████     | 25/50 [00:01<00:01, 16.60it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.53it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.55it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.61it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.64it/s] 70%|███████   | 35/50 [00:02<00:00, 16.67it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.68it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.69it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.70it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.72it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.73it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.68it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.52it/s]100%|██████████| 50/50 [00:03<00:00, 16.31it/s]
Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.
2023-10-30 13:40:12.475 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography panda eating banana, text output Sure, it is[SEG] .
2023-10-30 13:40:12.476 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/c7c78d2c-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@762.966] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:40:17.827 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography panda eating banana [/INST]
2023-10-30 13:40:20.072 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 256])
2023-10-30 13:40:20.072 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:40:20.175 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:40:20.177 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:40:20.178 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-10-30 13:40:20.178 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:40:20.288 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:40:20.290 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:40:20.290 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-10-30 13:40:20.290 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:40:20.475 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography panda eating banana input_image:  None
image_id:  cea50f3e-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  8.03it/s]  6%|▌         | 3/50 [00:00<00:03, 13.07it/s] 10%|█         | 5/50 [00:00<00:03, 14.69it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.38it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.79it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.09it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.29it/s] 30%|███       | 15/50 [00:00<00:02, 16.42it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.51it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.57it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.62it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.65it/s] 50%|█████     | 25/50 [00:01<00:01, 16.67it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.67it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.67it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.49it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.56it/s] 70%|███████   | 35/50 [00:02<00:00, 16.61it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.52it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.46it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.50it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.53it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.59it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.44it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.54it/s]100%|██████████| 50/50 [00:03<00:00, 16.22it/s]
2023-10-30 13:40:23.690 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography panda eating banana, text output Sure, it is[SEG] .
2023-10-30 13:40:23.690 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/cea50f3e-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@801.925] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:40:56.794 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography panda eating grass [/INST]
2023-10-30 13:40:59.068 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-10-30 13:40:59.068 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-10-30 13:40:59.172 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
2023-10-30 13:40:59.173 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:40:59.174 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-10-30 13:40:59.174 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-10-30 13:40:59.288 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
2023-10-30 13:40:59.290 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:40:59.290 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-10-30 13:40:59.290 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-10-30 13:40:59.475 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
input_str:  generate commercial photography panda eating grass input_image:  None
image_id:  e5ddbbe2-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.36it/s]  6%|▌         | 3/50 [00:00<00:03, 13.34it/s] 10%|█         | 5/50 [00:00<00:03, 14.95it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.71it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.10it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.28it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.43it/s] 30%|███       | 15/50 [00:00<00:02, 16.50it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.42it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.48it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.52it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.47it/s] 50%|█████     | 25/50 [00:01<00:01, 16.56it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.64it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.64it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.67it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.70it/s] 70%|███████   | 35/50 [00:02<00:00, 16.72it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.71it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.69it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.67it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.65it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.56it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.53it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.60it/s]100%|██████████| 50/50 [00:03<00:00, 16.34it/s]
2023-10-30 13:41:02.665 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography panda eating grass, text output Sure, it is[SEG] .
2023-10-30 13:41:02.666 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/e5ddbbe2-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@817.201] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:41:12.067 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography panda eating human [/INST]
2023-10-30 13:41:13.683 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-10-30 13:41:13.683 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-10-30 13:41:13.776 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
2023-10-30 13:41:13.778 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:41:13.778 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-10-30 13:41:13.778 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-10-30 13:41:13.887 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
2023-10-30 13:41:13.889 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:41:13.890 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-10-30 13:41:13.890 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-10-30 13:41:14.075 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
input_str:  generate commercial photography panda eating human input_image:  None
image_id:  eef895f8-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.38it/s]  6%|▌         | 3/50 [00:00<00:03, 13.32it/s] 10%|█         | 5/50 [00:00<00:03, 14.93it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.67it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.08it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.29it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.44it/s] 30%|███       | 15/50 [00:00<00:02, 16.54it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.59it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.64it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.68it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.70it/s] 50%|█████     | 25/50 [00:01<00:01, 16.73it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.74it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.75it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.77it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.79it/s] 70%|███████   | 35/50 [00:02<00:00, 16.79it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.79it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.79it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.80it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.80it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.80it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.72it/s] 98%|█████████▊| 49/50 [00:02<00:00, 16.68it/s]100%|██████████| 50/50 [00:03<00:00, 16.43it/s]
2023-10-30 13:41:17.247 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography panda eating human, text output Sure,[SEG] .
2023-10-30 13:41:17.248 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/eef895f8-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@837.180] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:41:32.087 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography panda eating a person [/INST]
2023-10-30 13:41:33.738 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-10-30 13:41:33.738 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:41:33.777 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:41:33.778 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:41:33.779 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-10-30 13:41:33.779 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:41:33.889 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-10-30 13:41:33.890 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:41:33.890 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-10-30 13:41:33.890 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-10-30 13:41:34.077 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  generate commercial photography panda eating a person input_image:  None
image_id:  fae12cae-76e6-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:07,  6.71it/s]  6%|▌         | 3/50 [00:00<00:03, 12.12it/s] 10%|█         | 5/50 [00:00<00:03, 14.15it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.17it/s] 18%|█▊        | 9/50 [00:00<00:02, 14.80it/s] 22%|██▏       | 11/50 [00:00<00:02, 14.92it/s] 26%|██▌       | 13/50 [00:00<00:02, 15.49it/s] 30%|███       | 15/50 [00:01<00:02, 15.74it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.06it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.26it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.37it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.27it/s] 50%|█████     | 25/50 [00:01<00:01, 16.43it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.54it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.60it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.66it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.69it/s] 70%|███████   | 35/50 [00:02<00:00, 16.72it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.55it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.62it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.66it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.49it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.47it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.44it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.39it/s]100%|██████████| 50/50 [00:03<00:00, 15.96it/s]
2023-10-30 13:41:37.345 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography panda eating a person, text output Sure,[SEG] .
2023-10-30 13:41:37.346 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/fae12cae-76e6-11ee-99f1-1070fd901efa.png
[ WARN:0@866.030] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:42:00.898 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
generate commercial photography panda dancing [/INST]
2023-10-30 13:42:02.540 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 256])
2023-10-30 13:42:02.541 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-10-30 13:42:02.583 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
2023-10-30 13:42:02.584 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-10-30 13:42:02.585 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 5120])
2023-10-30 13:42:02.585 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-10-30 13:42:02.687 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
2023-10-30 13:42:02.689 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:42:02.689 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 5120])
2023-10-30 13:42:02.689 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-10-30 13:42:02.873 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture
input_str:  generate commercial photography panda dancing input_image:  None
image_id:  0c134a0c-76e7-11ee-99f1-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  8.08it/s]  6%|▌         | 3/50 [00:00<00:03, 13.14it/s] 10%|█         | 5/50 [00:00<00:03, 14.80it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.58it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.01it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.28it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.39it/s] 30%|███       | 15/50 [00:00<00:02, 16.45it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.57it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.64it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.69it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.72it/s] 50%|█████     | 25/50 [00:01<00:01, 16.64it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.70it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.70it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.58it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.42it/s] 70%|███████   | 35/50 [00:02<00:00, 16.45it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.55it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.57it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.31it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.45it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.55it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.63it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.68it/s]100%|██████████| 50/50 [00:03<00:00, 16.30it/s]
2023-10-30 13:42:06.070 | INFO     | one_model.inference.infer:predict:33 - prompt generate commercial photography panda dancing, text output Sure,[SEG] .
2023-10-30 13:42:06.070 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/0c134a0c-76e7-11ee-99f1-1070fd901efa.png
2023-10-30 13:44:10.387 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe the picture [/INST]
2023-10-30 13:44:38.701 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 406, 256])
2023-10-30 13:44:38.702 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 406])
2023-10-30 13:44:39.075 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the picture
2023-10-30 13:44:39.515 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:44:39.547 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 406, 5120])
2023-10-30 13:44:39.548 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 406])
2023-10-30 13:44:39.581 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the picture
2023-10-30 13:44:39.583 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:44:39.583 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 406, 5120])
2023-10-30 13:44:39.584 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 406])
2023-10-30 13:44:39.768 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the picture
2023-10-30 13:44:39.770 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-10-30 13:44:39.770 | INFO     | one_model.inference.infer:predict:33 - prompt describe the picture, text output The image depicts a family enjoying a meal together at a dining table. There are several people seated around the table, with some of them actively eating. The table is filled with various food items, including a large salad, carrots, and a bowl of fruit. The atmosphere appears to be warm and inviting, as the family spends quality time together.
[ WARN:0@1025.083] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:45:47.872 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
how many person are in the picture? [/INST]
2023-10-30 13:45:50.730 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 256])
2023-10-30 13:45:50.730 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:45:50.872 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:45:51.278 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:45:51.314 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-10-30 13:45:51.315 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:45:51.386 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:45:51.387 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:45:51.388 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-10-30 13:45:51.388 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-10-30 13:45:51.581 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:45:51.582 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-10-30 13:45:51.583 | INFO     | one_model.inference.infer:predict:33 - prompt how many person are in the picture?, text output There are five people in the picture.
[ WARN:0@1096.895] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-10-30 13:46:10.081 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
list the food you find on the table [/INST]
2023-10-30 13:46:34.567 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 401, 256])
2023-10-30 13:46:34.567 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 401])
2023-10-30 13:46:34.678 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:46:35.119 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-10-30 13:46:35.151 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 401, 5120])
2023-10-30 13:46:35.151 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 401])
2023-10-30 13:46:35.186 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:46:35.187 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-10-30 13:46:35.188 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 401, 5120])
2023-10-30 13:46:35.188 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 401])
2023-10-30 13:46:35.376 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-10-30 13:46:35.377 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-10-30 13:46:35.378 | INFO     | one_model.inference.infer:predict:33 - prompt list the food you find on the table, text output The table is filled with a variety of food, including a large salad, a plate of carrots, a plate of broccoli, a plate of meat, and a bowl of fruit. There are also several cups and a bottle on the table, suggesting that beverages are available for the family to enjoy along with their meal.
[ WARN:0@1140.695] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
[ WARN:128@161391.148] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:17:26.060 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
truck tralier [/INST]
2023-11-01 10:17:28.454 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-01 10:17:28.454 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-01 10:17:29.486 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-01 10:17:29.991 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-01 10:17:30.081 | INFO     | one_model.inference.infer:predict:33 - prompt truck tralier, text output Sure, it is[SEG] .
2023-11-01 10:17:30.082 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/cc807d20-785c-11ee-82b7-1070fd901efa.png
2023-11-01 10:20:02.382 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe visible details about these objects and describe the interrelationships among these objects. [/INST]
2023-11-01 10:20:56.644 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 491, 256])
2023-11-01 10:20:56.645 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 491])
2023-11-01 10:20:57.985 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-01 10:20:58.459 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-01 10:20:58.465 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 491, 5120])
2023-11-01 10:20:58.466 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 491])
2023-11-01 10:20:59.078 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-01 10:20:59.088 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:20:59.088 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 491, 5120])
2023-11-01 10:20:59.089 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 491])
2023-11-01 10:20:59.389 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-01 10:20:59.474 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-11-01 10:20:59.479 | INFO     | one_model.inference.infer:predict:33 - prompt describe visible details about these objects and describe the interrelationships among these objects., text output In the image, there is a man smoking a cigarette while standing in a room. The room has a mirror, a vase, and a potted plant. The man is standing near the mirror, and the vase and potted plant are placed in the room, possibly on a surface or a shelf. The presence of the mirror, vase, and potted plant suggests that the room is decorated and designed to create a pleasant and aesthetically pleasing environment. The man's act of smoking a cigarette, however, contrasts with the serene atmosphere created by the other objects in the room. This juxtaposition highlights the tension between the man's action and the surrounding environment.
[ WARN:128@161604.792] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:22:18.313 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Segment the man. [/INST]
2023-11-01 10:22:20.601 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-01 10:22:20.601 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-01 10:22:21.676 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-01 10:22:22.150 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-01 10:22:22.166 | INFO     | one_model.inference.infer:predict:33 - prompt Segment the man., text output Sure, it is[SEG] .
2023-11-01 10:22:22.167 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/7ac9616c-785d-11ee-82b7-1070fd901efa.png
2023-11-01 10:22:52.256 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Segment the cigarette. [/INST]
2023-11-01 10:22:54.589 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-11-01 10:22:54.589 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-01 10:22:54.684 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-01 10:22:55.124 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-01 10:22:55.142 | INFO     | one_model.inference.infer:predict:33 - prompt Segment the cigarette., text output Sure, it is[SEG] .
2023-11-01 10:22:55.142 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/8f08035e-785d-11ee-82b7-1070fd901efa.png
2023-11-01 10:23:18.462 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Segment the hair. [/INST]
2023-11-01 10:23:20.076 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 328, 256])
2023-11-01 10:23:20.076 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 328])
2023-11-01 10:23:20.277 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-01 10:23:20.710 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-01 10:23:20.723 | INFO     | one_model.inference.infer:predict:33 - prompt Segment the hair., text output Sure,[SEG] .
2023-11-01 10:23:20.723 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/9ea63dd0-785d-11ee-82b7-1070fd901efa.png
2023-11-01 10:24:19.550 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of an empty flatbed [/INST]
2023-11-01 10:24:21.214 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-11-01 10:24:21.214 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-01 10:24:21.389 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:24:21.390 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:24:21.391 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-11-01 10:24:21.391 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-01 10:24:21.677 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:24:21.678 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:24:21.678 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-11-01 10:24:21.679 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-01 10:24:22.180 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  describe the picture input_image:  /tmp/gradio/b6a397fcf0e9977944ce5001c714cf89d7fb1380/image.png
image_id:  593ff23a-76e7-11ee-99f1-1070fd901efa
input_str:  how many person are in the picture? input_image:  /tmp/gradio/b6a397fcf0e9977944ce5001c714cf89d7fb1380/image.png
image_id:  93595dbc-76e7-11ee-99f1-1070fd901efa
input_str:  list the food you find on the table input_image:  /tmp/gradio/b6a397fcf0e9977944ce5001c714cf89d7fb1380/image.png
image_id:  a0979430-76e7-11ee-99f1-1070fd901efa
input_str:  truck tralier input_image:  None
image_id:  cc807d20-785c-11ee-82b7-1070fd901efa
input_str:  describe visible details about these objects and describe the interrelationships among these objects. input_image:  /tmp/gradio/94b27752900588a64fc15eba2b23cb1c9be9e780/image.png
image_id:  29c4acfe-785d-11ee-82b7-1070fd901efa
input_str:  Segment the man. input_image:  /tmp/gradio/94b27752900588a64fc15eba2b23cb1c9be9e780/image.png
image_id:  7ac9616c-785d-11ee-82b7-1070fd901efa
input_str:  Segment the cigarette. input_image:  /tmp/gradio/94b27752900588a64fc15eba2b23cb1c9be9e780/image.png
image_id:  8f08035e-785d-11ee-82b7-1070fd901efa
input_str:  Segment the hair. input_image:  /tmp/gradio/94b27752900588a64fc15eba2b23cb1c9be9e780/image.png
image_id:  9ea63dd0-785d-11ee-82b7-1070fd901efa
input_str:  Generate a image of an empty flatbed input_image:  /tmp/gradio/94b27752900588a64fc15eba2b23cb1c9be9e780/image.png
image_id:  c30fd514-785d-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:08,  5.63it/s]  6%|▌         | 3/50 [00:00<00:04, 10.90it/s] 10%|█         | 5/50 [00:00<00:03, 13.32it/s] 14%|█▍        | 7/50 [00:00<00:02, 14.59it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.30it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.66it/s] 26%|██▌       | 13/50 [00:00<00:02, 15.86it/s] 30%|███       | 15/50 [00:01<00:02, 16.11it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.30it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.46it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.24it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.41it/s] 50%|█████     | 25/50 [00:01<00:01, 16.53it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.62it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.58it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.64it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.12it/s] 70%|███████   | 35/50 [00:02<00:00, 16.21it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.40it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.52it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.59it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.40it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.42it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.51it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.39it/s]100%|██████████| 50/50 [00:03<00:00, 15.87it/s]
2023-11-01 10:24:25.461 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of an empty flatbed, text output Sure,[SEG] .
2023-11-01 10:24:25.461 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/c30fd514-785d-11ee-82b7-1070fd901efa.png
[ WARN:128@161847.188] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:25:02.061 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of an man kiss woman. [/INST]
2023-11-01 10:25:04.382 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 256])
2023-11-01 10:25:04.382 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-11-01 10:25:04.867 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:25:04.868 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:25:04.869 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-11-01 10:25:04.869 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-11-01 10:25:05.286 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:25:05.287 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:25:05.288 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-11-01 10:25:05.288 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-11-01 10:25:05.888 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate a image of an man kiss woman. input_image:  None
image_id:  dc52afec-785d-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  7.49it/s]  6%|▌         | 3/50 [00:00<00:03, 12.48it/s] 10%|█         | 5/50 [00:00<00:03, 14.25it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.21it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.62it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.02it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.06it/s] 30%|███       | 15/50 [00:00<00:02, 15.86it/s] 34%|███▍      | 17/50 [00:01<00:02, 15.95it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.07it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.21it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.19it/s] 50%|█████     | 25/50 [00:01<00:01, 16.19it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.32it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.47it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.59it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.67it/s] 70%|███████   | 35/50 [00:02<00:00, 16.71it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.74it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.77it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.78it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.79it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.79it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.78it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.78it/s]100%|██████████| 50/50 [00:03<00:00, 16.15it/s]
2023-11-01 10:25:09.193 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of an man kiss woman., text output Sure, it is[SEG] .
2023-11-01 10:25:09.193 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/dc52afec-785d-11ee-82b7-1070fd901efa.png
[ WARN:128@161866.386] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:25:21.255 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of a truck tralier. [/INST]
2023-11-01 10:25:23.518 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 256])
2023-11-01 10:25:23.519 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:25:23.772 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:25:23.773 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:25:23.774 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 5120])
2023-11-01 10:25:23.774 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:25:24.088 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:25:24.089 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:25:24.089 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 5120])
2023-11-01 10:25:24.090 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:25:24.683 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate a image of a truck tralier. input_image:  None
image_id:  e7c4083a-785d-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  7.82it/s]  6%|▌         | 3/50 [00:00<00:03, 12.57it/s] 10%|█         | 5/50 [00:00<00:03, 14.21it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.06it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.56it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.84it/s] 26%|██▌       | 13/50 [00:00<00:02, 15.96it/s] 30%|███       | 15/50 [00:00<00:02, 16.16it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.31it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.42it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.52it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.61it/s] 50%|█████     | 25/50 [00:01<00:01, 16.62it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.68it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.72it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.74it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.71it/s] 70%|███████   | 35/50 [00:02<00:00, 16.73it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.52it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.46it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.54it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.56it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.61it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.65it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.68it/s]100%|██████████| 50/50 [00:03<00:00, 16.19it/s]
2023-11-01 10:25:27.917 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of a truck tralier., text output Sure, it is[SEG] .
2023-11-01 10:25:27.917 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/e7c4083a-785d-11ee-82b7-1070fd901efa.png
[ WARN:128@161967.963] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:27:02.835 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of a sheep stand on a car. [/INST]
2023-11-01 10:27:05.134 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 256])
2023-11-01 10:27:05.134 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:27:05.478 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:27:05.480 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:27:05.480 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 5120])
2023-11-01 10:27:05.480 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:27:05.976 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:27:05.978 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:27:05.978 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 5120])
2023-11-01 10:27:05.978 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:27:06.568 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate a image of a sheep stand on a car. input_image:  None
image_id:  244f7aa0-785e-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.27it/s]  6%|▌         | 3/50 [00:00<00:03, 12.90it/s] 10%|█         | 5/50 [00:00<00:03, 14.62it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.47it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.91it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.20it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.41it/s] 30%|███       | 15/50 [00:00<00:02, 16.50it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.52it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.59it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.66it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.71it/s] 50%|█████     | 25/50 [00:01<00:01, 16.75it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.73it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.47it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.41it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.51it/s] 70%|███████   | 35/50 [00:02<00:00, 16.50it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.31it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.45it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.42it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.51it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.54it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.60it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.63it/s]100%|██████████| 50/50 [00:03<00:00, 16.26it/s]
2023-11-01 10:27:09.786 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of a sheep stand on a car., text output Sure, it is[SEG] .
2023-11-01 10:27:09.787 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/244f7aa0-785e-11ee-82b7-1070fd901efa.png
[ WARN:128@162012.301] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:27:47.173 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of a dog stand on a car. [/INST]
2023-11-01 10:27:49.462 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 256])
2023-11-01 10:27:49.463 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:27:49.787 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:27:49.788 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:27:49.789 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 5120])
2023-11-01 10:27:49.789 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:27:50.779 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:27:50.781 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:27:50.781 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 337, 5120])
2023-11-01 10:27:50.781 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 337])
2023-11-01 10:27:51.383 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate a image of a dog stand on a car. input_image:  None
image_id:  3ebcec06-785e-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  7.87it/s]  6%|▌         | 3/50 [00:00<00:04, 11.25it/s] 10%|█         | 5/50 [00:00<00:03, 13.31it/s] 14%|█▍        | 7/50 [00:00<00:02, 14.54it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.23it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.66it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.00it/s] 30%|███       | 15/50 [00:01<00:02, 16.24it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.41it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.54it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.63it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.31it/s] 50%|█████     | 25/50 [00:01<00:01, 16.45it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.49it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.53it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.51it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.58it/s] 70%|███████   | 35/50 [00:02<00:00, 16.50it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.60it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.67it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.71it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.65it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.49it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.28it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.42it/s]100%|██████████| 50/50 [00:03<00:00, 16.03it/s]
2023-11-01 10:27:54.631 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of a dog stand on a car., text output Sure, it is[SEG] .
2023-11-01 10:27:54.632 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/3ebcec06-785e-11ee-82b7-1070fd901efa.png
[ WARN:128@162050.967] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:28:25.840 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of some pigs stand on a car. [/INST]
2023-11-01 10:28:28.131 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 338, 256])
2023-11-01 10:28:28.131 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 338])
2023-11-01 10:28:28.273 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:28:28.274 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:28:28.274 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 338, 5120])
2023-11-01 10:28:28.275 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 338])
2023-11-01 10:28:28.769 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:28:28.771 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:28:28.771 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 338, 5120])
2023-11-01 10:28:28.771 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 338])
2023-11-01 10:28:29.067 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate a image of some pigs stand on a car. input_image:  None
image_id:  55c8ccf8-785e-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  8.12it/s]  6%|▌         | 3/50 [00:00<00:03, 12.92it/s] 10%|█         | 5/50 [00:00<00:03, 14.63it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.46it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.94it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.12it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.33it/s] 30%|███       | 15/50 [00:00<00:02, 16.33it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.31it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.25it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.19it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.16it/s] 50%|█████     | 25/50 [00:01<00:01, 16.14it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.21it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.39it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.52it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.60it/s] 70%|███████   | 35/50 [00:02<00:00, 16.67it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.62it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.62it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.40it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.29it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.34it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.34it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.24it/s]100%|██████████| 50/50 [00:03<00:00, 16.10it/s]
2023-11-01 10:28:32.312 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of some pigs stand on a car., text output Sure, it is[SEG] .
2023-11-01 10:28:32.312 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/55c8ccf8-785e-11ee-82b7-1070fd901efa.png
[ WARN:128@162159.409] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:30:14.296 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
a pig drive the car [/INST]
2023-11-01 10:30:17.646 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-11-01 10:30:17.646 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-01 10:30:17.777 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment all the object
2023-11-01 10:30:17.778 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:30:17.778 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-01 10:30:17.778 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-01 10:30:17.981 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment all the object
2023-11-01 10:30:17.982 | INFO     | one_model.model.decoder.openseed_decoder:forward:136 - handle image ./vis_input/966bd1ce-785e-11ee-82b7-1070fd901efa.png
/home/luban/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-11-01 10:30:19.458 | INFO     | one_model.inference.infer:predict:33 - prompt a pig drive the car, text output Sure, the segmentation result is[SEG] .
2023-11-01 10:30:19.458 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/966bd1ce-785e-11ee-82b7-1070fd901efa.png
[ WARN:128@162183.992] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-01 10:30:38.867 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image a pig drive the car [/INST]
2023-11-01 10:30:41.222 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 256])
2023-11-01 10:30:41.222 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-11-01 10:30:41.478 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:30:41.479 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-01 10:30:41.480 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-11-01 10:30:41.480 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-11-01 10:30:42.082 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-01 10:30:42.084 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-01 10:30:42.084 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 335, 5120])
2023-11-01 10:30:42.084 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 335])
2023-11-01 10:30:42.466 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  a pig drive the car input_image:  None
image_id:  966bd1ce-785e-11ee-82b7-1070fd901efa
input_str:  Generate a image a pig drive the car input_image:  None
image_id:  a512d934-785e-11ee-82b7-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  8.12it/s]  6%|▌         | 3/50 [00:00<00:03, 13.11it/s] 10%|█         | 5/50 [00:00<00:03, 14.79it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.58it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.03it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.29it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.39it/s] 30%|███       | 15/50 [00:00<00:02, 16.50it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.55it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.59it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.66it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.70it/s] 50%|█████     | 25/50 [00:01<00:01, 16.74it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.71it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.42it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.57it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.56it/s] 70%|███████   | 35/50 [00:02<00:00, 16.50it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.52it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.45it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.56it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.39it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.45it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.35it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.42it/s]100%|██████████| 50/50 [00:03<00:00, 16.26it/s]
2023-11-01 10:30:45.667 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image a pig drive the car, text output Sure, it is[SEG] .
2023-11-01 10:30:45.668 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/a512d934-785e-11ee-82b7-1070fd901efa.png
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
2023-11-07 05:02:10.278 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the man? [/INST]
2023-11-07 05:02:12.852 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 256])
2023-11-07 05:02:12.852 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-11-07 05:02:13.187 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:02:13.825 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:02:13.909 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the man?, text output Sure, it is[SEG] .
2023-11-07 05:02:13.909 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/c05a793a-7ce7-11ee-a961-1070fd901efa.png
2023-11-07 05:03:03.115 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the cigarette? [/INST]
2023-11-07 05:03:05.453 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-11-07 05:03:05.453 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:03:05.574 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:03:06.034 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:03:06.094 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the cigarette?, text output Sure, it is[SEG] .
2023-11-07 05:03:06.094 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/dfd907ae-7ce7-11ee-a961-1070fd901efa.png
2023-11-07 05:03:31.006 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment cigarette? [/INST]
2023-11-07 05:03:33.272 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-11-07 05:03:33.273 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-07 05:03:33.467 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment all the object
2023-11-07 05:03:33.468 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:03:33.469 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 5120])
2023-11-07 05:03:33.469 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-07 05:03:33.574 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment all the object
2023-11-07 05:03:33.575 | INFO     | one_model.model.decoder.openseed_decoder:forward:136 - handle image ./vis_input/f078f074-7ce7-11ee-a961-1070fd901efa.png
/home/luban/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-11-07 05:03:34.924 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment cigarette?, text output Sure, it is[SEG] .
2023-11-07 05:03:34.925 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/f078f074-7ce7-11ee-a961-1070fd901efa.png
2023-11-07 05:03:48.262 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the cigarette? [/INST]
2023-11-07 05:03:50.523 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-11-07 05:03:50.523 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:03:50.887 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:03:51.561 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:03:51.615 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the cigarette?, text output Sure, it is[SEG] .
2023-11-07 05:03:51.616 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/fac3c478-7ce7-11ee-a961-1070fd901efa.png
2023-11-07 05:04:16.322 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
segment the cigarette [/INST]
2023-11-07 05:04:18.573 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-07 05:04:18.573 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-07 05:04:18.682 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-07 05:04:19.115 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:04:19.170 | INFO     | one_model.inference.infer:predict:33 - prompt segment the cigarette, text output Sure, it is[SEG] .
2023-11-07 05:04:19.170 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/0b7d552c-7ce8-11ee-a961-1070fd901efa.png
2023-11-07 05:07:15.887 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
segment the cigarette [/INST]
2023-11-07 05:07:18.140 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-07 05:07:18.140 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-07 05:07:18.188 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-07 05:07:18.620 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:07:18.629 | INFO     | one_model.inference.infer:predict:33 - prompt segment the cigarette, text output Sure, it is[SEG] .
2023-11-07 05:07:18.630 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/768ef906-7ce8-11ee-a961-1070fd901efa.png
2023-11-07 05:07:31.070 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
segment the man [/INST]
2023-11-07 05:07:33.314 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 328, 256])
2023-11-07 05:07:33.314 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 328])
2023-11-07 05:07:33.372 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-07 05:07:33.732 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:07:33.743 | INFO     | one_model.inference.infer:predict:33 - prompt segment the man, text output Sure, it is[SEG] .
2023-11-07 05:07:33.743 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/7f9be338-7ce8-11ee-a961-1070fd901efa.png
2023-11-07 05:12:27.009 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the man? [/INST]
2023-11-07 05:12:29.265 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 256])
2023-11-07 05:12:29.265 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-11-07 05:12:29.379 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:12:29.812 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:12:29.824 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the man?, text output Sure, it is[SEG] .
2023-11-07 05:12:29.824 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/30006960-7ce9-11ee-a961-1070fd901efa.png
2023-11-07 05:14:40.570 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the man? [/INST]
2023-11-07 05:14:42.849 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 256])
2023-11-07 05:14:42.849 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-11-07 05:14:42.883 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:14:43.306 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:14:43.318 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the man?, text output Sure, it is[SEG] .
2023-11-07 05:14:43.318 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/7f9b2f0a-7ce9-11ee-a961-1070fd901efa.png
2023-11-07 05:15:08.262 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
segment the cigarette [/INST]
2023-11-07 05:15:10.521 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-07 05:15:10.521 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-07 05:15:10.568 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-07 05:15:10.922 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:15:10.931 | INFO     | one_model.inference.infer:predict:33 - prompt segment the cigarette, text output Sure, it is[SEG] .
2023-11-07 05:15:10.931 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/901c0eb2-7ce9-11ee-a961-1070fd901efa.png
2023-11-07 05:16:07.323 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe visible details about these objects and describe the interrelationships among these objects. [/INST]
2023-11-07 05:16:49.738 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 464, 256])
2023-11-07 05:16:49.738 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 464])
2023-11-07 05:16:50.088 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:16:50.540 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:16:50.544 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 464, 5120])
2023-11-07 05:16:50.545 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 464])
2023-11-07 05:16:50.675 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:16:50.684 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:16:50.684 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 464, 5120])
2023-11-07 05:16:50.684 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 464])
2023-11-07 05:16:50.891 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:16:50.971 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-11-07 05:16:50.972 | INFO     | one_model.inference.infer:predict:33 - prompt describe visible details about these objects and describe the interrelationships among these objects., text output In the image, there is a man sitting in a chair, smoking a cigarette. The chair is positioned near a dining table, and a potted plant is placed on the table. The man is enjoying his cigarette, which is the main focus of the scene. The dining table and potted plant provide a setting for the man to relax and smoke, creating a casual and comfortable atmosphere. The presence of the potted plant adds a touch of greenery and natural elements to the scene, while the dining table suggests that the area might be used for dining or socializing.
[ WARN:129@661756.284] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-07 05:21:44.066 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the red car? [/INST]
2023-11-07 05:21:46.296 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-11-07 05:21:46.296 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-07 05:21:46.377 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:21:46.813 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:21:46.838 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the red car?, text output Sure, it is[SEG] .
2023-11-07 05:21:46.838 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/7bffdb92-7cea-11ee-a961-1070fd901efa.png
2023-11-07 05:22:08.667 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the man? [/INST]
2023-11-07 05:22:10.923 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 331, 256])
2023-11-07 05:22:10.923 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 331])
2023-11-07 05:22:10.971 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:22:11.321 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:22:11.332 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the man?, text output Sure, it is[SEG] .
2023-11-07 05:22:11.332 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/8aafe36c-7cea-11ee-a961-1070fd901efa.png
2023-11-07 05:22:31.059 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
segment the cigarette [/INST]
2023-11-07 05:22:33.271 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-07 05:22:33.271 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-07 05:22:33.368 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment the
2023-11-07 05:22:33.720 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:22:33.729 | INFO     | one_model.inference.infer:predict:33 - prompt segment the cigarette, text output Sure, it is[SEG] .
2023-11-07 05:22:33.729 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/980affb0-7cea-11ee-a961-1070fd901efa.png
2023-11-07 05:23:08.095 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment everything in this image? [/INST]
2023-11-07 05:23:10.349 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-11-07 05:23:10.349 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:23:10.483 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment everything
2023-11-07 05:23:10.484 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:23:10.485 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-11-07 05:23:10.485 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:23:10.677 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment everything
2023-11-07 05:23:10.678 | INFO     | one_model.model.decoder.openseed_decoder:forward:136 - handle image ./vis_input/adeab42e-7cea-11ee-a961-1070fd901efa.png
/home/luban/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-11-07 05:23:14.262 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment everything in this image?, text output Sure, it is[SEG] .
2023-11-07 05:23:14.263 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/adeab42e-7cea-11ee-a961-1070fd901efa.png
2023-11-07 05:23:40.956 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe visible details about these objects and describe the interrelationships among these objects. [/INST]
2023-11-07 05:24:52.173 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 256])
2023-11-07 05:24:52.173 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-11-07 05:24:52.785 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:24:53.667 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:24:53.732 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 5120])
2023-11-07 05:24:53.732 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-11-07 05:24:53.872 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:24:53.874 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:24:53.874 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 5120])
2023-11-07 05:24:53.874 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-11-07 05:24:54.076 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:24:54.080 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-11-07 05:24:54.080 | INFO     | one_model.inference.infer:predict:33 - prompt describe visible details about these objects and describe the interrelationships among these objects., text output In the image, there are several objects visible on the road, including a large truck, a car, a pile of tires, and a traffic light. The truck is the most prominent vehicle on the road, taking up a significant portion of the scene. The car is positioned behind the truck, and the traffic light is located near the left side of the image. The pile of tires is situated on the right side of the road, possibly indicating a tire repair or disposal area.The interrelationship among these objects suggests that the truck and car are both using the road for transportation, while the traffic light helps to regulate the flow of traffic. The presence of the pile of tires indicates that there might be a nearby tire repair or waste disposal facility, which could be related to the truck or other vehicles on the road. Overall, the scene depicts a typical day on a road with various vehicles and objects in motion.
[ WARN:129@662239.393] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
[ WARN:129@662305.624] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-07 05:26:00.498 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of a little cat. [/INST]
2023-11-07 05:26:02.816 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-11-07 05:26:02.817 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:26:02.888 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:26:02.966 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:26:02.967 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-07 05:26:02.967 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:26:03.175 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:26:03.177 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:26:03.177 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-07 05:26:03.177 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:26:03.385 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  can you segment the man? input_image:  /tmp/gradio/5d68a10ed416e71c81e92d6ea54f194a7e691321/image.png
image_id:  c05a793a-7ce7-11ee-a961-1070fd901efa
input_str:  can you segment the cigarette? input_image:  /tmp/gradio/5d68a10ed416e71c81e92d6ea54f194a7e691321/image.png
image_id:  dfd907ae-7ce7-11ee-a961-1070fd901efa
input_str:  can you segment cigarette? input_image:  /tmp/gradio/5d68a10ed416e71c81e92d6ea54f194a7e691321/image.png
image_id:  f078f074-7ce7-11ee-a961-1070fd901efa
input_str:  can you segment the cigarette? input_image:  /tmp/gradio/5d68a10ed416e71c81e92d6ea54f194a7e691321/image.png
image_id:  fac3c478-7ce7-11ee-a961-1070fd901efa
input_str:  segment the cigarette input_image:  /tmp/gradio/5d68a10ed416e71c81e92d6ea54f194a7e691321/image.png
image_id:  0b7d552c-7ce8-11ee-a961-1070fd901efa
input_str:  segment the cigarette input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  768ef906-7ce8-11ee-a961-1070fd901efa
input_str:  segment the man input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  7f9be338-7ce8-11ee-a961-1070fd901efa
input_str:  can you segment the man? input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  30006960-7ce9-11ee-a961-1070fd901efa
input_str:  can you segment the man? input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  7f9b2f0a-7ce9-11ee-a961-1070fd901efa
input_str:  segment the cigarette input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  901c0eb2-7ce9-11ee-a961-1070fd901efa
input_str:  describe visible details about these objects and describe the interrelationships among these objects. input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  b351b166-7ce9-11ee-a961-1070fd901efa
input_str:  can you segment the red car? input_image:  /tmp/gradio/e65b82bfd61931cbb99eabcb89ce75a31ba12c98/image.png
image_id:  7bffdb92-7cea-11ee-a961-1070fd901efa
input_str:  can you segment the man? input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  8aafe36c-7cea-11ee-a961-1070fd901efa
input_str:  segment the cigarette input_image:  /tmp/gradio/718cb181d20f9022eceacb701654cb2740b03433/image.png
image_id:  980affb0-7cea-11ee-a961-1070fd901efa
input_str:  can you segment everything in this image? input_image:  /tmp/gradio/05e1b79505597aa67cf8316375fc11920797a924/image.png
image_id:  adeab42e-7cea-11ee-a961-1070fd901efa
input_str:  describe visible details about these objects and describe the interrelationships among these objects. input_image:  /tmp/gradio/05e1b79505597aa67cf8316375fc11920797a924/image.png
image_id:  c1855a48-7cea-11ee-a961-1070fd901efa
input_str:  Generate a image of a little cat. input_image:  None
image_id:  14cbc41c-7ceb-11ee-a961-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:08,  5.55it/s]  6%|▌         | 3/50 [00:00<00:04, 11.04it/s] 10%|█         | 5/50 [00:00<00:03, 13.42it/s] 14%|█▍        | 7/50 [00:00<00:02, 14.69it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.44it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.77it/s] 26%|██▌       | 13/50 [00:00<00:02, 15.97it/s] 30%|███       | 15/50 [00:01<00:02, 16.24it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.44it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.56it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.65it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.72it/s] 50%|█████     | 25/50 [00:01<00:01, 16.76it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.79it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.79it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.79it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.80it/s] 70%|███████   | 35/50 [00:02<00:00, 16.73it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.76it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.77it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.78it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.78it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.80it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.80it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.80it/s]100%|██████████| 50/50 [00:03<00:00, 16.13it/s]
2023-11-07 05:26:06.686 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of a little cat., text output Sure, it is[SEG] .
2023-11-07 05:26:06.686 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/14cbc41c-7ceb-11ee-a961-1070fd901efa.png
[ WARN:129@662329.275] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-07 05:26:24.141 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate an image of a truck. [/INST]
2023-11-07 05:26:26.458 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-11-07 05:26:26.459 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:26:26.580 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:26:26.581 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:26:26.581 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-07 05:26:26.582 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:26:26.768 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:26:26.770 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:26:26.770 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-07 05:26:26.770 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:26:26.880 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate an image of a truck. input_image:  None
image_id:  22e4a366-7ceb-11ee-a961-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.45it/s]  6%|▌         | 3/50 [00:00<00:03, 13.38it/s] 10%|█         | 5/50 [00:00<00:03, 14.97it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.72it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.93it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.24it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.40it/s] 30%|███       | 15/50 [00:00<00:02, 16.55it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.62it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.66it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.69it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.73it/s] 50%|█████     | 25/50 [00:01<00:01, 16.75it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.76it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.73it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.68it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.71it/s] 70%|███████   | 35/50 [00:02<00:00, 16.67it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.69it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.72it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.55it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.64it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.65it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.69it/s] 98%|█████████▊| 49/50 [00:02<00:00, 16.73it/s]100%|██████████| 50/50 [00:03<00:00, 16.40it/s]
2023-11-07 05:26:30.053 | INFO     | one_model.inference.infer:predict:33 - prompt Generate an image of a truck., text output Sure, it is[SEG] .
2023-11-07 05:26:30.054 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/22e4a366-7ceb-11ee-a961-1070fd901efa.png
[ WARN:129@662362.793] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-07 05:26:57.659 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate an image of an empty flatbed. [/INST]
2023-11-07 05:26:59.271 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-11-07 05:26:59.272 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:26:59.382 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:26:59.386 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:26:59.387 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-11-07 05:26:59.387 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:26:59.581 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:26:59.582 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:26:59.583 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-11-07 05:26:59.583 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:26:59.774 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate an image of an empty flatbed. input_image:  None
image_id:  36df14c8-7ceb-11ee-a961-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:05,  8.41it/s]  6%|▌         | 3/50 [00:00<00:03, 13.35it/s] 10%|█         | 5/50 [00:00<00:03, 14.93it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.66it/s] 18%|█▊        | 9/50 [00:00<00:02, 16.03it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.27it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.24it/s] 30%|███       | 15/50 [00:00<00:02, 16.35it/s] 34%|███▍      | 17/50 [00:01<00:02, 16.41it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.54it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.63it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.70it/s] 50%|█████     | 25/50 [00:01<00:01, 16.74it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.77it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.78it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.79it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.80it/s] 70%|███████   | 35/50 [00:02<00:00, 16.81it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.68it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.73it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.76it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.77it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.79it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.80it/s] 98%|█████████▊| 49/50 [00:02<00:00, 16.80it/s]100%|██████████| 50/50 [00:03<00:00, 16.42it/s]
2023-11-07 05:27:02.945 | INFO     | one_model.inference.infer:predict:33 - prompt Generate an image of an empty flatbed., text output Sure,[SEG] .
2023-11-07 05:27:02.945 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/36df14c8-7ceb-11ee-a961-1070fd901efa.png
[ WARN:129@662571.287] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-07 05:30:26.157 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate an image of a little cat. [/INST]
2023-11-07 05:30:28.460 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-11-07 05:30:28.460 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:30:28.575 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:30:28.576 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:30:28.576 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-07 05:30:28.576 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:30:28.768 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-07 05:30:28.770 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:30:28.770 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-07 05:30:28.770 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-07 05:30:28.885 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Generate an image of a little cat. input_image:  None
image_id:  b324b38a-7ceb-11ee-a961-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:08,  5.50it/s]  6%|▌         | 3/50 [00:00<00:04, 10.98it/s] 10%|█         | 5/50 [00:00<00:03, 13.38it/s] 14%|█▍        | 7/50 [00:00<00:02, 14.67it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.44it/s] 22%|██▏       | 11/50 [00:00<00:02, 15.92it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.22it/s] 30%|███       | 15/50 [00:01<00:02, 16.44it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.57it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.66it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.72it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.76it/s] 50%|█████     | 25/50 [00:01<00:01, 16.78it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.80it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.81it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.80it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.81it/s] 70%|███████   | 35/50 [00:02<00:00, 16.82it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.81it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.82it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.83it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.84it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.84it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.85it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.85it/s]100%|██████████| 50/50 [00:03<00:00, 16.18it/s]
2023-11-07 05:30:32.096 | INFO     | one_model.inference.infer:predict:33 - prompt Generate an image of a little cat., text output Sure, it is[SEG] .
2023-11-07 05:30:32.096 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/b324b38a-7ceb-11ee-a961-1070fd901efa.png
2023-11-07 05:31:10.803 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Can you segment everything in this image? [/INST]
2023-11-07 05:31:13.044 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-11-07 05:31:13.044 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:31:13.085 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment everything
2023-11-07 05:31:13.087 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-07 05:31:13.087 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-11-07 05:31:13.087 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-07 05:31:13.276 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment everything
2023-11-07 05:31:13.277 | INFO     | one_model.model.decoder.openseed_decoder:forward:136 - handle image ./vis_input/cda7f910-7ceb-11ee-a961-1070fd901efa.png
/home/luban/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-11-07 05:31:16.601 | INFO     | one_model.inference.infer:predict:33 - prompt Can you segment everything in this image?, text output Sure, it is[SEG] .
2023-11-07 05:31:16.602 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/cda7f910-7ceb-11ee-a961-1070fd901efa.png
2023-11-07 05:31:44.447 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe visible details about these objects and describe the interrelationships among these objects. [/INST]
2023-11-07 05:32:56.425 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 256])
2023-11-07 05:32:56.425 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-11-07 05:32:56.672 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:32:57.532 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:32:57.597 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 5120])
2023-11-07 05:32:57.597 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-11-07 05:32:57.780 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:32:57.790 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-07 05:32:57.791 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 544, 5120])
2023-11-07 05:32:57.791 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 544])
2023-11-07 05:32:58.072 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-07 05:32:58.083 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-11-07 05:32:58.083 | INFO     | one_model.inference.infer:predict:33 - prompt describe visible details about these objects and describe the interrelationships among these objects., text output In the image, there are several objects visible on the road, including a large truck, a car, a pile of tires, and a traffic light. The truck is the most prominent vehicle on the road, taking up a significant portion of the scene. The car is positioned behind the truck, and the traffic light is located near the left side of the image. The pile of tires is situated on the right side of the road, possibly indicating a tire repair or disposal area.The interrelationship among these objects suggests that the truck and car are both using the road for transportation, while the traffic light helps to regulate the flow of traffic. The presence of the pile of tires indicates that there might be a nearby tire repair or waste disposal facility, which could be related to the truck or other vehicles on the road. Overall, the scene depicts a typical day on a road with various vehicles and objects in motion.
[ WARN:129@662723.396] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
2023-11-07 05:35:30.559 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the red car? [/INST]
2023-11-07 05:35:32.898 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 332, 256])
2023-11-07 05:35:32.898 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 332])
2023-11-07 05:35:32.988 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:35:33.499 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:35:33.523 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the red car?, text output Sure, it is[SEG] .
2023-11-07 05:35:33.523 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/68a2b4d2-7cec-11ee-a961-1070fd901efa.png
2023-11-07 05:35:54.866 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
can you segment the blue car? [/INST]
2023-11-07 05:35:56.460 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 330, 256])
2023-11-07 05:35:56.460 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 330])
2023-11-07 05:35:56.570 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment a
2023-11-07 05:35:56.931 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-07 05:35:56.952 | INFO     | one_model.inference.infer:predict:33 - prompt can you segment the blue car?, text output Sure,[SEG] .
2023-11-07 05:35:56.952 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/7720ca58-7cec-11ee-a961-1070fd901efa.png
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/routes.py", line 442, in run_predict
    output = await app.get_blocks().process_api(
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1395, in process_api
    data = self.postprocess_data(fn_index, result["prediction"], state)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/blocks.py", line 1329, in postprocess_data
    prediction_value = block.postprocess(prediction_value)
  File "/home/luban/.local/lib/python3.8/site-packages/gradio/components/image.py", line 320, in postprocess
    raise ValueError("Cannot process this value as an Image")
ValueError: Cannot process this value as an Image
2023-11-09 05:34:54.676 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Can you segment everything in this image? [/INST]
2023-11-09 05:34:56.987 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 256])
2023-11-09 05:34:56.988 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-09 05:34:57.367 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment everything
2023-11-09 05:34:57.369 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-09 05:34:57.369 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 333, 5120])
2023-11-09 05:34:57.369 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 333])
2023-11-09 05:34:57.482 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt segment everything
2023-11-09 05:34:57.483 | INFO     | one_model.model.decoder.openseed_decoder:forward:136 - handle image ./vis_input/a7e970b6-7e7e-11ee-acc0-1070fd901efa.png
/home/luban/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
2023-11-09 05:35:00.959 | INFO     | one_model.inference.infer:predict:33 - prompt Can you segment everything in this image?, text output Sure, it is[SEG] .
2023-11-09 05:35:00.959 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/a7e970b6-7e7e-11ee-acc0-1070fd901efa.png
2023-11-09 05:36:02.031 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
Generate a image of an empty flatbed [/INST]
2023-11-09 05:36:04.373 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 256])
2023-11-09 05:36:04.373 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-09 05:36:05.185 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-09 05:36:05.187 | INFO     | one_model.model.decoder.sam_decoder:forward:118 - skip sam decoder infer
2023-11-09 05:36:05.187 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-09 05:36:05.187 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-09 05:36:05.375 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
2023-11-09 05:36:05.376 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-09 05:36:05.377 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 334, 5120])
2023-11-09 05:36:05.377 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 334])
2023-11-09 05:36:05.490 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt generate a picture of
input_str:  Can you segment everything in this image? input_image:  /tmp/gradio/05e1b79505597aa67cf8316375fc11920797a924/image.png
image_id:  cda7f910-7ceb-11ee-a961-1070fd901efa
input_str:  describe visible details about these objects and describe the interrelationships among these objects. input_image:  /tmp/gradio/05e1b79505597aa67cf8316375fc11920797a924/image.png
image_id:  e1b1e10a-7ceb-11ee-a961-1070fd901efa
input_str:  can you segment the red car? input_image:  /tmp/gradio/e65b82bfd61931cbb99eabcb89ce75a31ba12c98/image.png
image_id:  68a2b4d2-7cec-11ee-a961-1070fd901efa
input_str:  can you segment the blue car? input_image:  /tmp/gradio/e65b82bfd61931cbb99eabcb89ce75a31ba12c98/image.png
image_id:  7720ca58-7cec-11ee-a961-1070fd901efa
input_str:   input_image:  /tmp/gradio/929a9eac0a752ea089a17dc252328af15b5308f5/image.png
input_str:  Can you segment everything in this image? input_image:  /tmp/gradio/929a9eac0a752ea089a17dc252328af15b5308f5/image.png
image_id:  a7e970b6-7e7e-11ee-acc0-1070fd901efa
input_str:  Generate a image of an empty flatbed input_image:  /tmp/gradio/929a9eac0a752ea089a17dc252328af15b5308f5/image.png
image_id:  d00c19ae-7e7e-11ee-acc0-1070fd901efa
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [00:00<00:06,  7.74it/s]  6%|▌         | 3/50 [00:00<00:03, 12.87it/s] 10%|█         | 5/50 [00:00<00:03, 14.64it/s] 14%|█▍        | 7/50 [00:00<00:02, 15.46it/s] 18%|█▊        | 9/50 [00:00<00:02, 15.93it/s] 22%|██▏       | 11/50 [00:00<00:02, 16.19it/s] 26%|██▌       | 13/50 [00:00<00:02, 16.38it/s] 30%|███       | 15/50 [00:00<00:02, 16.48it/s] 34%|███▍      | 17/50 [00:01<00:01, 16.58it/s] 38%|███▊      | 19/50 [00:01<00:01, 16.65it/s] 42%|████▏     | 21/50 [00:01<00:01, 16.29it/s] 46%|████▌     | 23/50 [00:01<00:01, 16.46it/s] 50%|█████     | 25/50 [00:01<00:01, 16.54it/s] 54%|█████▍    | 27/50 [00:01<00:01, 16.62it/s] 58%|█████▊    | 29/50 [00:01<00:01, 16.68it/s] 62%|██████▏   | 31/50 [00:01<00:01, 16.65it/s] 66%|██████▌   | 33/50 [00:02<00:01, 16.69it/s] 70%|███████   | 35/50 [00:02<00:00, 16.71it/s] 74%|███████▍  | 37/50 [00:02<00:00, 16.70it/s] 78%|███████▊  | 39/50 [00:02<00:00, 16.61it/s] 82%|████████▏ | 41/50 [00:02<00:00, 16.66it/s] 86%|████████▌ | 43/50 [00:02<00:00, 16.69it/s] 90%|█████████ | 45/50 [00:02<00:00, 16.72it/s] 94%|█████████▍| 47/50 [00:02<00:00, 16.73it/s] 98%|█████████▊| 49/50 [00:03<00:00, 16.74it/s]100%|██████████| 50/50 [00:03<00:00, 16.31it/s]
2023-11-09 05:36:08.775 | INFO     | one_model.inference.infer:predict:33 - prompt Generate a image of an empty flatbed, text output Sure, it is[SEG] .
2023-11-09 05:36:08.775 | INFO     | one_model.inference.infer:predict:58 - save segment to ./vis_output/d00c19ae-7e7e-11ee-acc0-1070fd901efa.png
2023-11-09 05:36:36.343 | INFO     | one_model.model.one_model:generate_input_ids:316 - prompt [INST] <<SYS>>
You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.
<</SYS>>

<im_start><image><im_end>
describe visible details about these objects and describe the interrelationships among these objects [/INST]
2023-11-09 05:37:17.170 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 455, 256])
2023-11-09 05:37:17.170 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 455])
2023-11-09 05:37:17.785 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-09 05:37:18.624 | INFO     | one_model.model.decoder.sam_decoder:forward:179 - pred_masks 1
2023-11-09 05:37:18.692 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 455, 5120])
2023-11-09 05:37:18.692 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 455])
2023-11-09 05:37:18.875 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-09 05:37:18.876 | INFO     | one_model.model.decoder.openseed_decoder:forward:133 - skip openseed decoder infer
2023-11-09 05:37:18.877 | INFO     | one_model.model.one_model:generate:262 - hidden_states torch.Size([1, 455, 5120])
2023-11-09 05:37:18.877 | INFO     | one_model.model.one_model:generate:263 - seg_token_mask shape torch.Size([1, 455])
2023-11-09 05:37:19.070 | INFO     | one_model.model.decoder.decoder_selector:need_run:58 - best match prompt describe the photo
2023-11-09 05:37:19.072 | INFO     | one_model.model.decoder.sd_decoder:forward:45 - skip sd decoder infer
2023-11-09 05:37:19.072 | INFO     | one_model.inference.infer:predict:33 - prompt describe visible details about these objects and describe the interrelationships among these objects, text output In the image, there is a large truck driving down a highway, with a blue truck in the foreground and a red truck in the background. The trucks are traveling in the same direction, and they are surrounded by a large, open field. The trucks are the main focus of the scene, and their presence suggests that they are transporting goods or materials along the highway. The open field in the background provides a sense of the rural or remote location of the highway, and it highlights the road as the primary transportation route in the area.
[ WARN:130@835784.385] global loadsave.cpp:248 findDecoder imread_(''): can't open/read file: check file path/integrity
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 408, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
    return await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/fastapi/applications.py", line 292, in __call__
    await super().__call__(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/applications.py", line 122, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 184, in __call__
    raise exc
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 91, in __call__
    await response(scope, receive, sender)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/responses.py", line 164, in __call__
    await send(
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/exceptions.py", line 65, in sender
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/starlette/middleware/errors.py", line 159, in _send
    await send(message)
  File "/home/luban/.local/lib/python3.8/site-packages/uvicorn/protocols/http/h11_impl.py", line 490, in send
    output = self.conn.send(event=response)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 512, in send
    data_list = self.send_with_data_passthrough(event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 537, in send_with_data_passthrough
    self._process_event(self.our_role, event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_connection.py", line 272, in _process_event
    self._cstate.process_event(role, type(event), server_switch_event)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 293, in process_event
    self._fire_event_triggered_transitions(role, _event_type)
  File "/home/luban/.local/lib/python3.8/site-packages/h11/_state.py", line 311, in _fire_event_triggered_transitions
    raise LocalProtocolError(
h11._util.LocalProtocolError: can't handle event type Response when role=SERVER and state=MUST_CLOSE
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
